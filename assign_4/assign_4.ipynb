{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgJox5B8qcRckVW7JvL6pR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gc2321/3546-Deep-Learning/blob/main/assign_4/assign_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "shapes3d: https://www.tensorflow.org/datasets/catalog/shapes3d"
      ],
      "metadata": {
        "id": "zdUrxuHAz87O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "Praz63WE0p8L"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "tq_F4Kp72i_K"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info = tfds.builder('shapes3d').info"
      ],
      "metadata": {
        "id": "Tws-qVes2lW6"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l30YSupM2yeM",
        "outputId": "3f0a7e6c-f583-4d02-d151-85e96c1a805d"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='shapes3d',\n",
              "    full_name='shapes3d/2.0.0',\n",
              "    description=\"\"\"\n",
              "    3dshapes is a dataset of 3D shapes procedurally generated from 6 ground truth\n",
              "    independent latent factors. These factors are *floor colour*, *wall colour*,\n",
              "    *object colour*, *scale*, *shape* and *orientation*.\n",
              "    \n",
              "    All possible combinations of these latents are present exactly once, generating\n",
              "    N = 480000 total images.\n",
              "    \n",
              "    ### Latent factor values\n",
              "    \n",
              "    *   floor hue: 10 values linearly spaced in [0, 1]\n",
              "    *   wall hue: 10 values linearly spaced in [0, 1]\n",
              "    *   object hue: 10 values linearly spaced in [0, 1]\n",
              "    *   scale: 8 values linearly spaced in [0, 1]\n",
              "    *   shape: 4 values in [0, 1, 2, 3]\n",
              "    *   orientation: 15 values linearly spaced in [-30, 30]\n",
              "    \n",
              "    We varied one latent at a time (starting from orientation, then shape, etc), and\n",
              "    sequentially stored the images in fixed order in the `images` array. The\n",
              "    corresponding values of the factors are stored in the same order in the `labels`\n",
              "    array.\n",
              "    \"\"\",\n",
              "    homepage='https://github.com/deepmind/3d-shapes',\n",
              "    data_dir='/root/tensorflow_datasets/shapes3d/2.0.0',\n",
              "    file_format=tfrecord,\n",
              "    download_size=255.18 MiB,\n",
              "    dataset_size=1.68 GiB,\n",
              "    features=FeaturesDict({\n",
              "        'image': Image(shape=(64, 64, 3), dtype=uint8),\n",
              "        'label_floor_hue': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
              "        'label_object_hue': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
              "        'label_orientation': ClassLabel(shape=(), dtype=int64, num_classes=15),\n",
              "        'label_scale': ClassLabel(shape=(), dtype=int64, num_classes=8),\n",
              "        'label_shape': ClassLabel(shape=(), dtype=int64, num_classes=4),\n",
              "        'label_wall_hue': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
              "        'value_floor_hue': float32,\n",
              "        'value_object_hue': float32,\n",
              "        'value_orientation': float32,\n",
              "        'value_scale': float32,\n",
              "        'value_shape': float32,\n",
              "        'value_wall_hue': float32,\n",
              "    }),\n",
              "    supervised_keys=None,\n",
              "    disable_shuffling=False,\n",
              "    splits={\n",
              "        'train': <SplitInfo num_examples=480000, num_shards=16>,\n",
              "    },\n",
              "    citation=\"\"\"@misc{3dshapes18,\n",
              "      title={3D Shapes Dataset},\n",
              "      author={Burgess, Chris and Kim, Hyunjik},\n",
              "      howpublished={https://github.com/deepmind/3dshapes-dataset/},\n",
              "      year={2018}\n",
              "    }\"\"\",\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = info.splits['train'].num_examples\n",
        "# take 30% of data\n",
        "take_data = int(0.3 * total)\n",
        "train_dataset = tfds.load('shapes3d', split=f'train[:{take_data}]')"
      ],
      "metadata": {
        "id": "OIUwH1Jl22R_"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "take_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuY5OKY722OF",
        "outputId": "eb641789-d9c4-40e4-a53f-3432217315bb"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "144000"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "    image = tf.cast(data['image'], tf.float32) / 255.0\n",
        "    return image"
      ],
      "metadata": {
        "id": "gzY-hjU122GN"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(preprocess_data)\n",
        "train_dataset = train_dataset.batch(batch_size=32, drop_remainder=True)\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "px_gkeh776nI"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variational Autoencoder"
      ],
      "metadata": {
        "id": "ENKf4e2f0bcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/tutorials/generative/cvae#network_architecture\n",
        "\n",
        "class CVAE(tf.keras.Model):\n",
        "    \"\"\"Convolutional variational autoencoder.\"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim):\n",
        "        \"\"\"\n",
        "        latent_dim: int, typically much smaller than the original input dimension and represent the compressed, encoded version of the data.\n",
        "        \"\"\"\n",
        "        super(CVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = tf.keras.Sequential(\n",
        "            [\n",
        "                tf.keras.layers.InputLayer(input_shape=(64, 64, 3)),\n",
        "                # 4 layers\n",
        "                tf.keras.layers.Conv2D(\n",
        "                    filters=32, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
        "                ),\n",
        "                tf.keras.layers.Conv2D(\n",
        "                    filters=64, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
        "                ),\n",
        "                tf.keras.layers.Conv2D(\n",
        "                    filters=128, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
        "                ),\n",
        "                tf.keras.layers.Conv2D(\n",
        "                    filters=256, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
        "                ),\n",
        "                tf.keras.layers.Flatten(),\n",
        "                # No activation\n",
        "                tf.keras.layers.Dense(latent_dim + latent_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.decoder = tf.keras.Sequential(\n",
        "            [\n",
        "                tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
        "                tf.keras.layers.Dense(units=4 * 4 * 256, activation=tf.nn.relu),\n",
        "                tf.keras.layers.Reshape(target_shape=(4, 4, 256)),\n",
        "                tf.keras.layers.Conv2DTranspose(\n",
        "                    filters=128,\n",
        "                    kernel_size=3,\n",
        "                    strides=2,\n",
        "                    padding=\"same\",\n",
        "                    activation=\"relu\",\n",
        "                ),\n",
        "                tf.keras.layers.Conv2DTranspose(\n",
        "                    filters=64,\n",
        "                    kernel_size=3,\n",
        "                    strides=2,\n",
        "                    padding=\"same\",\n",
        "                    activation=\"relu\",\n",
        "                ),\n",
        "                tf.keras.layers.Conv2DTranspose(\n",
        "                    filters=32,\n",
        "                    kernel_size=3,\n",
        "                    strides=2,\n",
        "                    padding=\"same\",\n",
        "                    activation=\"relu\",\n",
        "                ),\n",
        "                # No activation\n",
        "                tf.keras.layers.Conv2DTranspose(\n",
        "                    filters=3, kernel_size=3, strides=2, padding=\"same\"\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def encode(self, x):\n",
        "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
        "        return mean, logvar\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        batch_size = tf.shape(mean)[0]  # Get batch size dynamically\n",
        "        eps = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        return eps * tf.exp(logvar * 0.5) + mean\n",
        "\n",
        "    def decode(self, z, apply_sigmoid=False):\n",
        "        logits = self.decoder(z)\n",
        "        if apply_sigmoid:\n",
        "            probs = tf.sigmoid(logits)\n",
        "            return probs\n",
        "        return logits\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        # Access the images using indexing\n",
        "        images = data[0]\n",
        "        with tf.GradientTape() as tape:\n",
        "            mean, logvar = self.encode(images)\n",
        "            z = self.reparameterize(mean, logvar)\n",
        "            reconstruction = self.decode(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    tf.keras.losses.binary_crossentropy(images, reconstruction),\n",
        "                    axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * tf.reduce_mean(\n",
        "                tf.reduce_sum(1 + logvar - tf.square(mean) - tf.exp(logvar), axis=1)\n",
        "            )\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }"
      ],
      "metadata": {
        "id": "Y_djd1WqED-q"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "OMCN68lV_vyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aSDz-GvvFcsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 2\n",
        "cvae = CVAE(latent_dim)\n",
        "cvae.compile(optimizer=tf.keras.optimizers.Adam())"
      ],
      "metadata": {
        "id": "qtHUeBLXA62B"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cvae.fit(train_dataset.batch(32), epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypXG_MTxDR7-",
        "outputId": "eed14b57-3bd8-462d-9325-cbc0ebe0b1ff"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "141/141 [==============================] - 74s 502ms/step - loss: 68.8787 - reconstruction_loss: 65.6413 - kl_loss: 0.0014\n",
            "Epoch 2/15\n",
            "141/141 [==============================] - 68s 478ms/step - loss: 64.2902 - reconstruction_loss: 64.2595 - kl_loss: 9.3000e-09\n",
            "Epoch 3/15\n",
            "141/141 [==============================] - 70s 493ms/step - loss: 64.2912 - reconstruction_loss: 64.2599 - kl_loss: 2.3250e-09\n",
            "Epoch 4/15\n",
            "141/141 [==============================] - 71s 504ms/step - loss: 64.2892 - reconstruction_loss: 64.2579 - kl_loss: 9.0041e-08\n",
            "Epoch 5/15\n",
            "141/141 [==============================] - 71s 504ms/step - loss: 64.2725 - reconstruction_loss: 64.2234 - kl_loss: 1.0304e-06\n",
            "Epoch 6/15\n",
            "141/141 [==============================] - 67s 477ms/step - loss: 64.0587 - reconstruction_loss: 63.9639 - kl_loss: 4.0236e-06\n",
            "Epoch 7/15\n",
            "141/141 [==============================] - 71s 502ms/step - loss: 63.9290 - reconstruction_loss: 63.9026 - kl_loss: 4.4989e-06\n",
            "Epoch 8/15\n",
            "141/141 [==============================] - 70s 497ms/step - loss: 63.9219 - reconstruction_loss: 63.8934 - kl_loss: 4.8629e-06\n",
            "Epoch 9/15\n",
            "141/141 [==============================] - 69s 487ms/step - loss: 63.9140 - reconstruction_loss: 63.8851 - kl_loss: 2.1517e-06\n",
            "Epoch 10/15\n",
            "141/141 [==============================] - 72s 511ms/step - loss: 63.9127 - reconstruction_loss: 63.8844 - kl_loss: 1.2065e-06\n",
            "Epoch 11/15\n",
            "141/141 [==============================] - 70s 497ms/step - loss: 63.9133 - reconstruction_loss: 63.8842 - kl_loss: 1.3559e-06\n",
            "Epoch 12/15\n",
            "141/141 [==============================] - 65s 463ms/step - loss: 63.9137 - reconstruction_loss: 63.8834 - kl_loss: 6.3938e-07\n",
            "Epoch 13/15\n",
            "141/141 [==============================] - 68s 482ms/step - loss: 63.9131 - reconstruction_loss: 63.8824 - kl_loss: 8.9174e-07\n",
            "Epoch 14/15\n",
            "141/141 [==============================] - 68s 481ms/step - loss: 63.9126 - reconstruction_loss: 63.8815 - kl_loss: 4.7198e-07\n",
            "Epoch 15/15\n",
            "141/141 [==============================] - 67s 472ms/step - loss: 63.9114 - reconstruction_loss: 63.8809 - kl_loss: 5.5103e-07\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79d7d500eaa0>"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Show 2D manifolds of the code"
      ],
      "metadata": {
        "id": "8i2i9zgzSs23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_latent_images(cvae, n, digit_size=64):\n",
        "    \"\"\"Plots n x n digit images decoded from the latent space.\"\"\"\n",
        "    grid_x = np.linspace(-3, 3, n)\n",
        "    grid_y = np.linspace(-3, 3, n)\n",
        "    image_width = digit_size * n\n",
        "    image_height = image_width\n",
        "    image = np.zeros((image_height, image_width))\n",
        "\n",
        "    for i, yi in enumerate(grid_x):\n",
        "        for j, xi in enumerate(grid_y):\n",
        "            z = np.array([[xi, yi]])\n",
        "            reconstruction = cvae.decode(z)\n",
        "            digit_size = int(reconstruction.shape[0] ** 0.5)\n",
        "            if digit_size ** 2 != reconstruction.shape[0]:\n",
        "                digit_size = reconstruction.shape[0]\n",
        "            digit = tf.reshape(reconstruction[0], (digit_size, digit_size))\n",
        "            image[i * digit_size: (i + 1) * digit_size,\n",
        "                  j * digit_size: (j + 1) * digit_size] = digit.numpy()\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(image, cmap='Greys_r')\n",
        "    plt.axis('Off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "cbpyt019DXPx"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_latent_images(cvae, 20)"
      ],
      "metadata": {
        "id": "ArHfX7tKS2y4",
        "outputId": "9dfd19db-2f16-4e7a-8246-6e694ece4b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 12288 values, but the requested shape has 1 [Op:Reshape]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-2d8f630fc02f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_latent_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-121-04c432ca70a1>\u001b[0m in \u001b[0;36mplot_latent_images\u001b[0;34m(cvae, n, digit_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdigit_size\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mreconstruction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mdigit_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreconstruction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mdigit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdigit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigit_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             image[i * digit_size: (i + 1) * digit_size,\n\u001b[1;32m     18\u001b[0m                   j * digit_size: (j + 1) * digit_size] = digit.numpy()\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 12288 values, but the requested shape has 1 [Op:Reshape]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ldf1ekQYTJt6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}