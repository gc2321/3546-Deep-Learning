{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SCS 3546: Deep Learning\n",
        "> **Assignment 3: Contextualized Word Embeddings**"
      ],
      "metadata": {
        "id": "N5vYuxQwmkFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your name & student number:\n",
        "\n",
        "<pre> Gordon Chan </pre>\n",
        "\n",
        "<pre> qq525548 </pre>"
      ],
      "metadata": {
        "id": "b1kme0NkmpLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Assignment Description**\n",
        "***\n",
        "\n",
        "Search Engines are a standard tool for finding relevant content. The calculation of similarity between textual information is an important factor for better search results.\n",
        "\n",
        "### **Objectives**\n",
        "\n",
        "**Your goal in this assignment is to calculate the textual similarity between queries and the provided sample documents, using a variety of NLP approaches.**\n",
        "\n",
        "In achieving the above goal, you will also:\n",
        "- Demonstrate how to preprocess text and embed textual data.\n",
        "- Compare the results of textual similarity scoring between traditional and deep-learning based NLP methods.\n",
        "\n",
        "### **Data and Queries**\n",
        "\n",
        "You will use the document repository provided by `sample_repository.json`, which you can download from the following link, or from the assignment description in Quercus: https://q.utoronto.ca/courses/286389/files/21993451/download?download_frd=1\n",
        "\n",
        "The queries you will run against these sample documents are the following:\n",
        "\n",
        "- Query 1: “fruits”\n",
        "- Query 2: “vegetables”\n",
        "- Query 3: “healthy foods in Canada”\n",
        "\n",
        "### **Techniques to Demonstrate**\n",
        "\n",
        "The techniques you will use to compute the similarity scores are:\n",
        "- 1. TF-IDF.\n",
        "- 2. Semantic similarity using GloVe word vectors.\n",
        "- 3. Semantic similarity using a BERT-based model.\n",
        "\n",
        "\n",
        "### **Feel Free to Choose Your Own Approach**\n",
        "\n",
        "How you go about demonstrating each of the above techniques is up to you. You are not expected to use any particular library. The code below is just meant to provide you with some guidance to get started. You **do**, however, need to demonstrate obtaining similarity scores **with all 3 techniques above**, but how you go about doing this is totally up to you. The evaluation will be based on your ability obtain results using all three techniques, plus your discussion/comparison of any differences you observe.\n",
        "\n"
      ],
      "metadata": {
        "id": "qtPkbEdymrmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Grade Allocation**\n",
        "***\n",
        "15 points total\n",
        "\n",
        "- Experiment 1 (TD-IDF), implementation: 2 marks\n",
        "- Experiment 2 (GloVe), implementation: 3 marks\n",
        "- Experiment 3 (BERT), implementation: 3 marks\n",
        "- Comparison and Discussion: 3 marks\n",
        "  - Compare all three techniques and interpret your findings. Do your best to explain the differences you observe in terms of concepts learned in class (not just the _what_, but also the _how_ and _why_ one technique produces different results from another).\n",
        "- Text Pre-Processing: 2 marks\n",
        " - Cleaning and standardization (e.g. lemmatization, stemming) in Experiment 1\n",
        " - Basic text cleaning (e.g. removal of special characters or tags) in Experiments 2 and 3.\n",
        "- Clarity: 2 marks\n",
        " - The marks for clarity are awarded for code documentation, clean code (e.g. avoiding repetition by building re-usable functions)  and how well you explained/supported your answers, including the use of visualizations.\n"
      ],
      "metadata": {
        "id": "RsGXPLNwm1MK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnvVdcQSiKMA"
      },
      "source": [
        "# Setup and Data Import\n",
        "***\n",
        "You can use the code snippets below to help you load and extract the document repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "psBH7U7K5wVB",
        "outputId": "8497a55c-6525-41c3-990a-d57485921080",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filePath =\"/content/gdrive/MyDrive/neural_data/sample_repository.json\""
      ],
      "metadata": {
        "id": "VNAZpAW75rIi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VlWbMwpg4u5"
      },
      "source": [
        "# this will unpack the json file contents into a list of titles and documents\n",
        "import json\n",
        "\n",
        "with open(filePath) as in_file:\n",
        "    repo_data = json.load(in_file)\n",
        "\n",
        "titles = [item[0] for item in repo_data['data']]\n",
        "documents = [item[1] for item in repo_data['data']]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's take a look at some of these documents and titles;\n",
        "# here we print the five last entries\n",
        "for id in range(-5, 0, 1):\n",
        "  print(f\"Document title: {titles[id]}\")\n",
        "  print(f\"Document contents: {documents[id]}\")\n",
        "  print(\"\\n\") # adds newline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgDwMaKYpdIS",
        "outputId": "c894c8f9-b381-4784-bcba-ab8839625c57"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document title: botany\n",
            "Document contents: Botany, also called plant science(s), plant biology or phytology, is the science of plant life and a branch of biology. A botanist, plant scientist or phytologist is a scientist who specialises in this field. \n",
            "\n",
            "\n",
            "Document title: Ford Bronco \n",
            "Document contents: The Ford Bronco is a model line of sport utility vehicles manufactured and marketed by Ford. ... The first SUV model developed by the company, five generations of the Bronco were sold from the 1966 to 1996 model years. A sixth generation of the model line is sold from the 2021 model year. the Ford Bronco will be available in Canada, with first deliveries beginning in spring of 2021. The Bronco will come in six versions in Canada: Base, Big Bend, Black Diamond, Outer Banks, Wildtrak and Badlands. \n",
            "\n",
            "\n",
            "Document title: List of fruit dishes\n",
            "Document contents: Fruit dishes are those that use fruit as a primary ingredient. Condiments prepared with fruit as a primary ingredient are also included in this list.\n",
            "\n",
            "\n",
            "Document title: Neuro linguistic programming\n",
            "Document contents: Neuro linguistic programming (NLP) is a pseudoscientific approach to communication, personal development, and psychotherapy created by Richard Bandler and John Grinder in California, United States, in the 1970s.\n",
            "\n",
            "\n",
            "Document title: fruit serving bowl\n",
            "Document contents: A fruit serving bowl is a round dish or container typically used to prepare and serve food. The interior of a bowl is characteristically shaped like a spherical cap, with the edges and the bottom forming a seamless curve. This makes bowls especially suited for holding liquids and loose food, as the contents of the bowl are naturally concentrated in its center by the force of gravity.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1zBu6DIjdtP"
      },
      "source": [
        "# Experiment 1: TF-IDF\n",
        "***\n",
        "\n",
        "**T**erm **F**requency - **I**nverse **D**ocument **F**requency (TF-IDF) is a traditional NLP technique to look at words that appear in both pieces of text, and score them based on how often they appear. For this experiment, you are free to use the TF-IDF implementation provided by scikit-learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XP9_ncYg4yu",
        "outputId": "d69ae096-f52d-43ad-ae50-30f0a771f712"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym076HWMg42r"
      },
      "source": [
        "query = 'fruits'\n",
        "# query  terms: 'fruits' / 'vegetables' / 'healthy foods in Canada'\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
        "vectors = vectorizer.fit_transform([query] + documents)\n",
        "\n",
        "# Calculate the word frequency, and a measure of similarity of the search terms with each document.\n",
        "# Do not apply any text pre-processing (i.e. cleanup) yet.\n",
        "\n",
        "\n",
        "# for each query, output the similarity scores for the top 5 documents with\n",
        "# th highest score, and interpret your results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqrN27J9mZwb"
      },
      "source": [
        "## Repeat the same task after some preprocessing\n",
        "\n",
        "Use a minimum of 2 different text cleaning/standardization techniques (e.g. lemmatization, removing punctuation, etc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSsdfQb9g46z"
      },
      "source": [
        "# e.g. you can use a lemmatizer to reduce words down to their\n",
        "# simplest 'lemma' (helpful when dealing with plurals)\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaHFkf8tnuJB"
      },
      "source": [
        "#!pip install tfidf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What impact did the text cleaning / preprocessing have on your results?"
      ],
      "metadata": {
        "id": "eg7AQ9wGy1BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your response here"
      ],
      "metadata": {
        "id": "wRWtek5oy5tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8V2kx6Epnxq"
      },
      "source": [
        "# Experiment 2: Semantic matching using GloVe embeddings\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGBg_Roo8FvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6816cad4-6208-4e89-f590-05faf26544fe"
      },
      "source": [
        "# if you decide to use the gensim library and the sample codes below,\n",
        "# you would need gensim version >=4.0.1 to be installed\n",
        "!pip install  gensim==4.0.1\n",
        "import gensim\n",
        "print(gensim.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==4.0.1\n",
            "  Downloading gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9 MB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.1) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.1) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.1) (5.2.1)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.1\n",
            "4.0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oywLsqBYrayZ"
      },
      "source": [
        "import logging\n",
        "import json\n",
        "import logging\n",
        "from re import sub\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gensim.downloader as api\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
        "from gensim.similarities import SparseTermSimilarityMatrix\n",
        "from gensim.similarities import SoftCosineSimilarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa0dlD102lbp"
      },
      "source": [
        "# optional, but it helps\n",
        "import logging\n",
        "\n",
        "# Initialize logging.\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwQBtaxk5rXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc873a9-262e-436d-93b8-bce72375971b"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# Import and download stopwords from NLTK.\n",
        "nltk.download('stopwords')  # Download stopwords list.\n",
        "stopwords = set(nltk.corpus.stopwords.words(\"english\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mlGyZeZ2Nle"
      },
      "source": [
        "def preprocess(doc):\n",
        "    # Tokenize, clean up input document string\n",
        "    doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n",
        "    # you may decide to add additional steps here\n",
        "    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(\"inf\")) if token not in stopwords]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP1s_4hMpqAn"
      },
      "source": [
        "# Load test data\n",
        "with open('sample_repository.json') as in_file:\n",
        "    repo_data = json.load(in_file)\n",
        "\n",
        "titles = [item[0] for item in repo_data['data']]\n",
        "documents = [item[1] for item in repo_data['data']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaVXOXGDpqD5"
      },
      "source": [
        "query_s = 'Your queries here'\n",
        "\n",
        "# Preprocess the documents, including the query string\n",
        "corpus = [preprocess(document) for document in documents]\n",
        "query = preprocess(query_s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR_F5zkipqH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5238a0d4-db83-48c0-ef99-965566c2fdb9"
      },
      "source": [
        "# Download and load the GloVe word vector embeddings\n",
        "if 'glove' not in locals():  # only load if not already in memory\n",
        "    glove = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "similarity_index = WordEmbeddingSimilarityIndex(glove)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtWEI622pqLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5822cfa4-606b-434c-f1bb-455801344b23"
      },
      "source": [
        "# Build the term dictionary, TF-idf model\n",
        "# Keep in mind that the search query must be in the dictionary as well, in case the terms do not overlap with the documents\n",
        "dictionary = Dictionary(corpus+[query])\n",
        "tfidf = TfidfModel(dictionary=dictionary)\n",
        "\n",
        "# Create the term similarity matrix.\n",
        "# The nonzero_limit enforces sparsity by limiting the number of non-zero terms in each column.\n",
        "# In my case, I got best results by removing the default value of 100\n",
        "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)  # , nonzero_limit=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 569/569 [00:25<00:00, 22.14it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn9tPObFpqPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47eef5c-dae4-4abf-cb00-f16facad770d"
      },
      "source": [
        "# Compute similarity measure between the query and the documents.\n",
        "query_tf = tfidf[dictionary.doc2bow(query)]\n",
        "\n",
        "index = SoftCosineSimilarity(\n",
        "            tfidf[[dictionary.doc2bow(document) for document in corpus]],\n",
        "            similarity_matrix)\n",
        "\n",
        "doc_similarity_scores = index[query_tf]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/termsim.py:359: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  normalized_corpus = np.multiply(corpus, 1.0 / corpus_norm)\n",
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/termsim.py:359: RuntimeWarning: invalid value encountered in multiply\n",
            "  normalized_corpus = np.multiply(corpus, 1.0 / corpus_norm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-XmAUHwpqSD"
      },
      "source": [
        "# for each query, output the similarity scores for the top 5 documents with\n",
        "# th highest score, and interpret your results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8at07gGAI-MH"
      },
      "source": [
        "# Experiment 3: BERT Model\n",
        "***\n",
        "Use a BERT model obtain sentence embeddings and calculate the similarity between queries and documents.\n",
        "\n",
        "> Hint: see the Module 07 jupyter notebook for examples of how to work with BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBSozs_tYFjg"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raaQrNXUKL2o"
      },
      "source": [
        "# for each query, output the similarity scores for the top 5 documents with\n",
        "# th highest score, and interpret your results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLH7jBj4KL7E"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Technique Comparison\n",
        " ***\n",
        "\n",
        "Compare all three techniques and interpret your findings. Do your best to explain the differences you observe in terms of concepts learned in class (not just the what, but also the how and why one technique produces different results from another).\n"
      ],
      "metadata": {
        "id": "XpX8H6_TEPOM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LwKekAZX1kDj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}