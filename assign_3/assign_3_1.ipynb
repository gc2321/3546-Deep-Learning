{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SCS 3546: Deep Learning\n",
        "> **Assignment 3: Contextualized Word Embeddings**"
      ],
      "metadata": {
        "id": "N5vYuxQwmkFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your name & student number:\n",
        "\n",
        "<pre> Gordon Chan </pre>\n",
        "\n",
        "<pre> qq525548 </pre>"
      ],
      "metadata": {
        "id": "b1kme0NkmpLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Assignment Description**\n",
        "***\n",
        "\n",
        "Search Engines are a standard tool for finding relevant content. The calculation of similarity between textual information is an important factor for better search results.\n",
        "\n",
        "### **Objectives**\n",
        "\n",
        "**Your goal in this assignment is to calculate the textual similarity between queries and the provided sample documents, using a variety of NLP approaches.**\n",
        "\n",
        "In achieving the above goal, you will also:\n",
        "- Demonstrate how to preprocess text and embed textual data.\n",
        "- Compare the results of textual similarity scoring between traditional and deep-learning based NLP methods.\n",
        "\n",
        "### **Data and Queries**\n",
        "\n",
        "You will use the document repository provided by `sample_repository.json`, which you can download from the following link, or from the assignment description in Quercus: https://q.utoronto.ca/courses/286389/files/21993451/download?download_frd=1\n",
        "\n",
        "The queries you will run against these sample documents are the following:\n",
        "\n",
        "- Query 1: “fruits”\n",
        "- Query 2: “vegetables”\n",
        "- Query 3: “healthy foods in Canada”\n",
        "\n",
        "### **Techniques to Demonstrate**\n",
        "\n",
        "The techniques you will use to compute the similarity scores are:\n",
        "- 1. TF-IDF.\n",
        "- 2. Semantic similarity using GloVe word vectors.\n",
        "- 3. Semantic similarity using a BERT-based model.\n",
        "\n",
        "\n",
        "### **Feel Free to Choose Your Own Approach**\n",
        "\n",
        "How you go about demonstrating each of the above techniques is up to you. You are not expected to use any particular library. The code below is just meant to provide you with some guidance to get started. You **do**, however, need to demonstrate obtaining similarity scores **with all 3 techniques above**, but how you go about doing this is totally up to you. The evaluation will be based on your ability obtain results using all three techniques, plus your discussion/comparison of any differences you observe.\n",
        "\n"
      ],
      "metadata": {
        "id": "qtPkbEdymrmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Grade Allocation**\n",
        "***\n",
        "15 points total\n",
        "\n",
        "- Experiment 1 (TD-IDF), implementation: 2 marks\n",
        "- Experiment 2 (GloVe), implementation: 3 marks\n",
        "- Experiment 3 (BERT), implementation: 3 marks\n",
        "- Comparison and Discussion: 3 marks\n",
        "  - Compare all three techniques and interpret your findings. Do your best to explain the differences you observe in terms of concepts learned in class (not just the _what_, but also the _how_ and _why_ one technique produces different results from another).\n",
        "- Text Pre-Processing: 2 marks\n",
        " - Cleaning and standardization (e.g. lemmatization, stemming) in Experiment 1\n",
        " - Basic text cleaning (e.g. removal of special characters or tags) in Experiments 2 and 3.\n",
        "- Clarity: 2 marks\n",
        " - The marks for clarity are awarded for code documentation, clean code (e.g. avoiding repetition by building re-usable functions)  and how well you explained/supported your answers, including the use of visualizations.\n"
      ],
      "metadata": {
        "id": "RsGXPLNwm1MK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnvVdcQSiKMA"
      },
      "source": [
        "# Setup and Data Import\n",
        "***\n",
        "You can use the code snippets below to help you load and extract the document repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psBH7U7K5wVB",
        "outputId": "5f0ffb7e-e1fd-481f-8415-e7d33148880d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filePath =\"/content/gdrive/MyDrive/neural_data/sample_repository.json\""
      ],
      "metadata": {
        "id": "VNAZpAW75rIi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VlWbMwpg4u5"
      },
      "source": [
        "# this will unpack the json file contents into a list of titles and documents\n",
        "import json\n",
        "\n",
        "with open(filePath) as in_file:\n",
        "    repo_data = json.load(in_file)\n",
        "\n",
        "titles = [item[0] for item in repo_data['data']]\n",
        "documents = [item[1] for item in repo_data['data']]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's take a look at some of these documents and titles;\n",
        "# here we print the five last entries\n",
        "for id in range(-5, 0, 1):\n",
        "  print(f\"Document title: {titles[id]}\")\n",
        "  print(f\"Document contents: {documents[id]}\")\n",
        "  print(\"\\n\") # adds newline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgDwMaKYpdIS",
        "outputId": "713cb601-e5ff-4b38-bd28-d017a9355736"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document title: botany\n",
            "Document contents: Botany, also called plant science(s), plant biology or phytology, is the science of plant life and a branch of biology. A botanist, plant scientist or phytologist is a scientist who specialises in this field. \n",
            "\n",
            "\n",
            "Document title: Ford Bronco \n",
            "Document contents: The Ford Bronco is a model line of sport utility vehicles manufactured and marketed by Ford. ... The first SUV model developed by the company, five generations of the Bronco were sold from the 1966 to 1996 model years. A sixth generation of the model line is sold from the 2021 model year. the Ford Bronco will be available in Canada, with first deliveries beginning in spring of 2021. The Bronco will come in six versions in Canada: Base, Big Bend, Black Diamond, Outer Banks, Wildtrak and Badlands. \n",
            "\n",
            "\n",
            "Document title: List of fruit dishes\n",
            "Document contents: Fruit dishes are those that use fruit as a primary ingredient. Condiments prepared with fruit as a primary ingredient are also included in this list.\n",
            "\n",
            "\n",
            "Document title: Neuro linguistic programming\n",
            "Document contents: Neuro linguistic programming (NLP) is a pseudoscientific approach to communication, personal development, and psychotherapy created by Richard Bandler and John Grinder in California, United States, in the 1970s.\n",
            "\n",
            "\n",
            "Document title: fruit serving bowl\n",
            "Document contents: A fruit serving bowl is a round dish or container typically used to prepare and serve food. The interior of a bowl is characteristically shaped like a spherical cap, with the edges and the bottom forming a seamless curve. This makes bowls especially suited for holding liquids and loose food, as the contents of the bowl are naturally concentrated in its center by the force of gravity.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titles[0], len(titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q71eg29aAAWa",
        "outputId": "12688ce3-6439-47db-cc01-6496a0416080"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Pomegranate Bhagwa', 32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0][:80], len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSceRGw6_xuc",
        "outputId": "a304c7cd-e474-4f92-c669-7975301cb2f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Fresh Pomegranate from Anushka Avni International Bhagwa is a premium Pomegranat',\n",
              " 32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1zBu6DIjdtP"
      },
      "source": [
        "# Experiment 1: TF-IDF\n",
        "***\n",
        "\n",
        "**T**erm **F**requency - **I**nverse **D**ocument **F**requency (TF-IDF) is a traditional NLP technique to look at words that appear in both pieces of text, and score them based on how often they appear. For this experiment, you are free to use the TF-IDF implementation provided by scikit-learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XP9_ncYg4yu",
        "outputId": "2af11bda-3326-441d-9200-7ead0113514d"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "stop_words = list(set(stopwords.words('english')))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Frequency in documents"
      ],
      "metadata": {
        "id": "QR6Jqz6sp7zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "1zY-E_KlozNu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use binary=True for sentences, not words\n",
        "cv = CountVectorizer(binary=True)\n",
        "\n",
        "X = cv.fit_transform(documents)"
      ],
      "metadata": {
        "id": "Bh-ShPBvpBJ-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW2dCVXnpBFy",
        "outputId": "bea7df30-495a-4732-d984-89bb14e08556"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fresh': 273,\n",
              " 'pomegranate': 483,\n",
              " 'from': 274,\n",
              " 'anushka': 70,\n",
              " 'avni': 88,\n",
              " 'international': 331,\n",
              " 'bhagwa': 112,\n",
              " 'is': 333,\n",
              " 'premium': 488,\n",
              " 'variety': 638,\n",
              " 'india': 320,\n",
              " 'the': 604,\n",
              " 'deep': 189,\n",
              " 'red': 521,\n",
              " 'arils': 79,\n",
              " 'pleasing': 478,\n",
              " 'but': 133,\n",
              " 'rugged': 535,\n",
              " 'skin': 563,\n",
              " 'enhances': 234,\n",
              " 'appearance': 71,\n",
              " 'whilst': 656,\n",
              " 'promoting': 502,\n",
              " 'shelf': 554,\n",
              " 'life': 357,\n",
              " 'of': 428,\n",
              " 'fruit': 275,\n",
              " 'widely': 661,\n",
              " 'known': 348,\n",
              " 'for': 264,\n",
              " 'its': 336,\n",
              " 'soft': 565,\n",
              " 'seed': 542,\n",
              " 'dark': 184,\n",
              " 'color': 160,\n",
              " 'and': 68,\n",
              " 'extremely': 246,\n",
              " 'delicious': 191,\n",
              " 'packaging': 453,\n",
              " 'net': 412,\n",
              " 'weight': 649,\n",
              " 'box': 128,\n",
              " '5kg': 47,\n",
              " '00kg': 1,\n",
              " 'details': 202,\n",
              " 'minimum': 400,\n",
              " '180gm': 11,\n",
              " 'maximum': 386,\n",
              " '400gm': 38,\n",
              " 'cherry': 151,\n",
              " 'taste': 599,\n",
              " 'sweet': 594,\n",
              " 'count': 174,\n",
              " 'carton': 145,\n",
              " '50': 43,\n",
              " 'kg': 345,\n",
              " 'wt': 666,\n",
              " 'numbers': 420,\n",
              " 'packed': 454,\n",
              " 'per': 467,\n",
              " '350': 36,\n",
              " '400gms': 39,\n",
              " 'arakta': 77,\n",
              " 'this': 609,\n",
              " 'are': 78,\n",
              " 'bigger': 114,\n",
              " 'in': 314,\n",
              " 'size': 560,\n",
              " 'with': 664,\n",
              " 'seeds': 544,\n",
              " 'bold': 121,\n",
              " 'it': 334,\n",
              " 'also': 65,\n",
              " 'possess': 485,\n",
              " 'glossy': 281,\n",
              " 'attractive': 84,\n",
              " '10': 2,\n",
              " '290': 30,\n",
              " '320gms': 33,\n",
              " '12': 4,\n",
              " '275': 27,\n",
              " '325gms': 34,\n",
              " '15': 5,\n",
              " '225': 24,\n",
              " '275gms': 28,\n",
              " 'load': 367,\n",
              " 'ability': 52,\n",
              " '4400': 41,\n",
              " 'cartons': 146,\n",
              " 'container': 170,\n",
              " '20': 17,\n",
              " 'pallets': 460,\n",
              " '220': 23,\n",
              " 'pallet': 459,\n",
              " '5500': 46,\n",
              " 'loading': 368,\n",
              " 'no': 416,\n",
              " 'availability': 86,\n",
              " 'january': 338,\n",
              " 'february': 250,\n",
              " 'march': 381,\n",
              " 'april': 75,\n",
              " 'july': 340,\n",
              " 'august': 85,\n",
              " 'september': 546,\n",
              " 'october': 426,\n",
              " 'nov': 418,\n",
              " 'dec': 187,\n",
              " 'read': 518,\n",
              " 'more': 404,\n",
              " 'about': 53,\n",
              " 'us': 628,\n",
              " 'aai': 51,\n",
              " 'takes': 597,\n",
              " 'pleasure': 479,\n",
              " 'presenting': 491,\n",
              " 'itself': 337,\n",
              " 'as': 80,\n",
              " 'one': 435,\n",
              " 'renowned': 526,\n",
              " 'suppliers': 587,\n",
              " 'exporter': 241,\n",
              " 'we': 647,\n",
              " 'have': 298,\n",
              " 'huge': 309,\n",
              " 'assortment': 82,\n",
              " 'agro': 62,\n",
              " 'products': 498,\n",
              " 'available': 87,\n",
              " 'feel': 251,\n",
              " 'proud': 505,\n",
              " 'when': 652,\n",
              " 'buyers': 134,\n",
              " 'come': 161,\n",
              " 'to': 616,\n",
              " 'recognizing': 519,\n",
              " 'standard': 575,\n",
              " 'quality': 516,\n",
              " 'which': 654,\n",
              " 'offer': 429,\n",
              " 'world': 665,\n",
              " 'wide': 660,\n",
              " 'market': 382,\n",
              " 'follow': 261,\n",
              " 'best': 110,\n",
              " 'practices': 486,\n",
              " 'while': 655,\n",
              " 'supplying': 589,\n",
              " 'white': 657,\n",
              " 'onions': 437,\n",
              " 'onion': 436,\n",
              " 'acclaimed': 57,\n",
              " 'health': 299,\n",
              " 'benefits': 108,\n",
              " 'used': 630,\n",
              " 'different': 211,\n",
              " 'cuisines': 179,\n",
              " 'major': 371,\n",
              " 'exporters': 242,\n",
              " 'all': 63,\n",
              " 'kinds': 347,\n",
              " 'like': 359,\n",
              " 'pink': 475,\n",
              " 'sizes': 562,\n",
              " '30': 31,\n",
              " '45': 42,\n",
              " 'mm': 401,\n",
              " '55': 45,\n",
              " 'above': 54,\n",
              " '70': 49,\n",
              " '60': 48,\n",
              " '3kg': 37,\n",
              " '9kg': 50,\n",
              " '10kg': 3,\n",
              " '15kg': 7,\n",
              " '17kg': 10,\n",
              " '18kg': 12,\n",
              " '20kg': 22,\n",
              " '25kg': 26,\n",
              " '30kg': 32,\n",
              " 'mesh': 394,\n",
              " 'bag': 92,\n",
              " 'or': 441,\n",
              " 'jute': 341,\n",
              " 'mt': 407,\n",
              " '20ft': 21,\n",
              " 'odo': 427,\n",
              " '28': 29,\n",
              " '40ft': 40,\n",
              " 'refer': 522,\n",
              " 'botanist': 123,\n",
              " 'an': 66,\n",
              " 'entity': 235,\n",
              " 'that': 603,\n",
              " 'develops': 206,\n",
              " 'fertilized': 252,\n",
              " 'ovary': 451,\n",
              " 'flower': 260,\n",
              " 'means': 388,\n",
              " 'tomatoes': 617,\n",
              " 'squash': 573,\n",
              " 'pumpkins': 509,\n",
              " 'cucumbers': 178,\n",
              " 'peppers': 466,\n",
              " 'eggplants': 228,\n",
              " 'corn': 173,\n",
              " 'kernels': 343,\n",
              " 'bean': 102,\n",
              " 'pea': 462,\n",
              " 'pods': 481,\n",
              " 'fruits': 276,\n",
              " 'so': 564,\n",
              " 'apples': 72,\n",
              " 'pears': 464,\n",
              " 'peaches': 463,\n",
              " 'apricots': 74,\n",
              " 'melons': 393,\n",
              " 'mangos': 376,\n",
              " 'nutrition': 423,\n",
              " 'biochemical': 116,\n",
              " 'physiological': 472,\n",
              " 'process': 494,\n",
              " 'by': 135,\n",
              " 'organism': 442,\n",
              " 'uses': 632,\n",
              " 'intake': 329,\n",
              " 'support': 590,\n",
              " 'includes': 316,\n",
              " 'ingestion': 325,\n",
              " 'absorption': 55,\n",
              " 'assimilation': 81,\n",
              " 'biosynthesis': 119,\n",
              " 'catabolism': 147,\n",
              " 'excretion': 240,\n",
              " 'science': 538,\n",
              " 'studies': 581,\n",
              " 'called': 137,\n",
              " 'nutritional': 424,\n",
              " 'nutrients': 422,\n",
              " 'substances': 583,\n",
              " 'survive': 592,\n",
              " 'grow': 292,\n",
              " 'reproduce': 528,\n",
              " 'seven': 550,\n",
              " 'classes': 155,\n",
              " 'relevant': 524,\n",
              " 'animals': 69,\n",
              " 'including': 317,\n",
              " 'humans': 310,\n",
              " 'carbohydrates': 142,\n",
              " 'dietary': 210,\n",
              " 'fiber': 253,\n",
              " 'fats': 249,\n",
              " 'proteins': 504,\n",
              " 'minerals': 399,\n",
              " 'vitamins': 644,\n",
              " 'water': 646,\n",
              " 'can': 138,\n",
              " 'be': 101,\n",
              " 'grouped': 291,\n",
              " 'either': 229,\n",
              " 'macronutrients': 370,\n",
              " 'needed': 411,\n",
              " 'gram': 285,\n",
              " 'quantities': 517,\n",
              " 'micronutrients': 397,\n",
              " 'milligram': 398,\n",
              " 'microgram': 396,\n",
              " 'diet': 209,\n",
              " 'sum': 585,\n",
              " 'foods': 263,\n",
              " 'eats': 222,\n",
              " 'largely': 352,\n",
              " 'determined': 203,\n",
              " 'palatability': 458,\n",
              " 'canada': 139,\n",
              " 'food': 262,\n",
              " 'guide': 293,\n",
              " 'produced': 496,\n",
              " 'promote': 501,\n",
              " 'healthy': 300,\n",
              " 'behaviours': 105,\n",
              " 'habits': 295,\n",
              " 'lifestyles': 358,\n",
              " 'increase': 319,\n",
              " 'number': 419,\n",
              " 'people': 465,\n",
              " '2007': 18,\n",
              " 'was': 645,\n",
              " 'reported': 527,\n",
              " 'second': 541,\n",
              " 'most': 406,\n",
              " 'requested': 530,\n",
              " 'canadian': 140,\n",
              " 'government': 283,\n",
              " 'publication': 508,\n",
              " 'behind': 106,\n",
              " 'income': 318,\n",
              " 'tax': 600,\n",
              " 'forms': 269,\n",
              " 'website': 648,\n",
              " 'states': 576,\n",
              " 'guides': 294,\n",
              " 'basic': 100,\n",
              " 'education': 226,\n",
              " 'tools': 618,\n",
              " 'designed': 200,\n",
              " 'help': 301,\n",
              " 'recommends': 520,\n",
              " 'eating': 221,\n",
              " 'each': 218,\n",
              " 'day': 186,\n",
              " 'plenty': 480,\n",
              " 'vegetables': 640,\n",
              " 'protein': 503,\n",
              " 'whole': 659,\n",
              " 'grain': 284,\n",
              " 'choosing': 152,\n",
              " 'plants': 477,\n",
              " 'often': 430,\n",
              " 'limiting': 361,\n",
              " 'highly': 303,\n",
              " 'processed': 495,\n",
              " 'uk': 622,\n",
              " 'nether': 413,\n",
              " 'land': 349,\n",
              " 'russia': 536,\n",
              " 'hongkong': 307,\n",
              " 'malaysia': 374,\n",
              " 'singapore': 556,\n",
              " 'oman': 433,\n",
              " 'uae': 621,\n",
              " 'qatar': 515,\n",
              " 'bahrain': 94,\n",
              " 'saudi': 537,\n",
              " 'arabia': 76,\n",
              " 'mauritius': 385,\n",
              " 'maldives': 375,\n",
              " 'sri': 574,\n",
              " 'lanka': 351,\n",
              " 'bangladesh': 96,\n",
              " 'industry': 323,\n",
              " 'statistics': 577,\n",
              " 'cis': 154,\n",
              " 'analyses': 67,\n",
              " 'data': 185,\n",
              " 'on': 434,\n",
              " 'many': 379,\n",
              " 'economic': 223,\n",
              " 'indicators': 322,\n",
              " 'sometimes': 567,\n",
              " 'purple': 512,\n",
              " 'cultivars': 180,\n",
              " 'purplish': 513,\n",
              " 'flesh': 259,\n",
              " 'tinged': 615,\n",
              " 'indian': 321,\n",
              " 'berry': 109,\n",
              " '18mm': 13,\n",
              " 'packing': 455,\n",
              " 'loose': 369,\n",
              " 'carry': 144,\n",
              " 'bags': 93,\n",
              " 'punnets10': 510,\n",
              " '500': 44,\n",
              " 'gms': 282,\n",
              " '2400': 25,\n",
              " 'kilo': 346,\n",
              " '3400': 35,\n",
              " '2040': 20,\n",
              " 'december': 188,\n",
              " 'summer': 586,\n",
              " 'exportingagro': 244,\n",
              " 'thoroughly': 611,\n",
              " 'consider': 168,\n",
              " 'demand': 195,\n",
              " 'requirement': 531,\n",
              " 'ofthe': 431,\n",
              " 'client': 156,\n",
              " 'till': 614,\n",
              " 'final': 255,\n",
              " 'destination': 201,\n",
              " 'company': 163,\n",
              " 'professionals': 499,\n",
              " 'aware': 89,\n",
              " 'fact': 248,\n",
              " 'demands': 196,\n",
              " 'thecustomers': 605,\n",
              " 'keep': 342,\n",
              " 'evolving': 239,\n",
              " 'do': 215,\n",
              " 'strong': 580,\n",
              " 'background': 90,\n",
              " 'horticultureand': 308,\n",
              " 'harvest': 297,\n",
              " 'handling': 296,\n",
              " 'perishables': 468,\n",
              " 'initiatives': 327,\n",
              " 'deliver': 192,\n",
              " 'moreover': 405,\n",
              " 'established': 237,\n",
              " 'ourselves': 448,\n",
              " 'forproviding': 270,\n",
              " 'across': 60,\n",
              " 'domestic': 217,\n",
              " 'internationalmarket': 332,\n",
              " 'leading': 354,\n",
              " 'organizations': 444,\n",
              " 'engaged': 232,\n",
              " 'delivering': 194,\n",
              " 'our': 447,\n",
              " 'customers': 183,\n",
              " 'manufacture': 377,\n",
              " 'bulk': 132,\n",
              " 'requirements': 532,\n",
              " 'clients': 157,\n",
              " 'philippines': 470,\n",
              " 'vietnam': 643,\n",
              " 'product': 497,\n",
              " 'emerged': 231,\n",
              " 'reputed': 529,\n",
              " 'organization': 443,\n",
              " 'actively': 61,\n",
              " 'participating': 461,\n",
              " 'exporting': 243,\n",
              " 'suppling': 588,\n",
              " 'enhancer': 233,\n",
              " 'free': 272,\n",
              " 'preservatives': 492,\n",
              " 'pure': 511,\n",
              " 'useful': 631,\n",
              " 'chutney': 153,\n",
              " '00': 0,\n",
              " 'welcome': 650,\n",
              " 'thomson': 610,\n",
              " 'seedless': 543,\n",
              " 'these': 607,\n",
              " 'grapes': 287,\n",
              " 'tart': 598,\n",
              " 'crunchy': 177,\n",
              " 'account': 59,\n",
              " 'table': 596,\n",
              " 'grape': 286,\n",
              " 'exports': 245,\n",
              " '16mm': 8,\n",
              " 'black': 120,\n",
              " 'sharad': 553,\n",
              " 'crisp': 176,\n",
              " 'vary': 639,\n",
              " 'shades': 551,\n",
              " 'usually': 633,\n",
              " 'medium': 392,\n",
              " 'sized': 561,\n",
              " 'oval': 450,\n",
              " 'shaped': 552,\n",
              " 'flame': 258,\n",
              " 'grapesflame': 288,\n",
              " 'popular': 484,\n",
              " 'varieties': 637,\n",
              " 'along': 64,\n",
              " 'dictionary': 208,\n",
              " 'python': 514,\n",
              " 'unordered': 627,\n",
              " 'collection': 158,\n",
              " 'values': 636,\n",
              " 'store': 578,\n",
              " 'map': 380,\n",
              " 'unlike': 626,\n",
              " 'other': 446,\n",
              " 'types': 619,\n",
              " 'hold': 304,\n",
              " 'only': 438,\n",
              " 'single': 557,\n",
              " 'value': 635,\n",
              " 'element': 230,\n",
              " 'holds': 306,\n",
              " 'key': 344,\n",
              " 'pair': 457,\n",
              " 'botany': 124,\n",
              " 'originated': 445,\n",
              " 'prehistory': 487,\n",
              " 'herbalism': 302,\n",
              " 'efforts': 227,\n",
              " 'early': 220,\n",
              " 'identify': 312,\n",
              " 'later': 353,\n",
              " 'cultivate': 181,\n",
              " 'edible': 225,\n",
              " 'medicinal': 390,\n",
              " 'poisonous': 482,\n",
              " 'making': 373,\n",
              " 'oldest': 432,\n",
              " 'branches': 130,\n",
              " 'medieval': 391,\n",
              " 'physic': 471,\n",
              " 'gardens': 278,\n",
              " 'attached': 83,\n",
              " 'monasteries': 403,\n",
              " 'contained': 169,\n",
              " 'medical': 389,\n",
              " 'importance': 313,\n",
              " 'they': 608,\n",
              " 'were': 651,\n",
              " 'forerunners': 267,\n",
              " 'first': 256,\n",
              " 'botanical': 122,\n",
              " 'universities': 625,\n",
              " 'founded': 271,\n",
              " '1540s': 6,\n",
              " 'onwards': 439,\n",
              " 'earliest': 219,\n",
              " 'padua': 456,\n",
              " 'garden': 277,\n",
              " 'facilitated': 247,\n",
              " 'academic': 56,\n",
              " 'study': 582,\n",
              " 'catalogue': 148,\n",
              " 'describe': 197,\n",
              " 'their': 606,\n",
              " 'collections': 159,\n",
              " 'beginnings': 104,\n",
              " 'plant': 476,\n",
              " 'taxonomy': 601,\n",
              " 'led': 355,\n",
              " '1753': 9,\n",
              " 'binomial': 115,\n",
              " 'system': 595,\n",
              " 'nomenclature': 417,\n",
              " 'carl': 143,\n",
              " 'linnaeus': 364,\n",
              " 'remains': 525,\n",
              " 'use': 629,\n",
              " 'naming': 408,\n",
              " 'biological': 117,\n",
              " 'species': 569,\n",
              " 'semantic': 545,\n",
              " 'similarity': 555,\n",
              " 'metric': 395,\n",
              " 'defined': 190,\n",
              " 'over': 452,\n",
              " 'set': 549,\n",
              " 'documents': 216,\n",
              " 'terms': 602,\n",
              " 'where': 653,\n",
              " 'idea': 311,\n",
              " 'distance': 214,\n",
              " 'between': 111,\n",
              " 'items': 335,\n",
              " 'based': 99,\n",
              " 'likeness': 360,\n",
              " 'meaning': 387,\n",
              " 'content': 171,\n",
              " 'opposed': 440,\n",
              " 'lexicographical': 356,\n",
              " 'mathematical': 384,\n",
              " 'estimate': 238,\n",
              " 'strength': 579,\n",
              " 'relationship': 523,\n",
              " 'units': 624,\n",
              " 'language': 350,\n",
              " 'concepts': 166,\n",
              " 'instances': 328,\n",
              " 'through': 613,\n",
              " 'numerical': 421,\n",
              " 'description': 199,\n",
              " 'obtained': 425,\n",
              " 'according': 58,\n",
              " 'comparison': 164,\n",
              " 'information': 324,\n",
              " 'supporting': 591,\n",
              " 'describing': 198,\n",
              " 'nature': 410,\n",
              " 'biology': 118,\n",
              " 'phytology': 474,\n",
              " 'branch': 129,\n",
              " 'scientist': 539,\n",
              " 'phytologist': 473,\n",
              " 'who': 658,\n",
              " 'specialises': 568,\n",
              " 'field': 254,\n",
              " 'ford': 266,\n",
              " 'bronco': 131,\n",
              " 'model': 402,\n",
              " 'line': 362,\n",
              " 'sport': 571,\n",
              " 'utility': 634,\n",
              " 'vehicles': 641,\n",
              " 'manufactured': 378,\n",
              " 'marketed': 383,\n",
              " 'suv': 593,\n",
              " 'developed': 204,\n",
              " 'five': 257,\n",
              " 'generations': 280,\n",
              " 'sold': 566,\n",
              " '1966': 14,\n",
              " '1996': 16,\n",
              " 'years': 668,\n",
              " 'sixth': 559,\n",
              " 'generation': 279,\n",
              " '2021': 19,\n",
              " 'year': 667,\n",
              " 'will': 663,\n",
              " 'deliveries': 193,\n",
              " 'beginning': 103,\n",
              " 'spring': 572,\n",
              " 'six': 558,\n",
              " 'versions': 642,\n",
              " 'base': 98,\n",
              " 'big': 113,\n",
              " 'bend': 107,\n",
              " 'diamond': 207,\n",
              " 'outer': 449,\n",
              " 'banks': 97,\n",
              " 'wildtrak': 662,\n",
              " 'badlands': 91,\n",
              " 'dishes': 213,\n",
              " 'those': 612,\n",
              " 'primary': 493,\n",
              " 'ingredient': 326,\n",
              " 'condiments': 167,\n",
              " 'prepared': 490,\n",
              " 'included': 315,\n",
              " 'list': 366,\n",
              " 'neuro': 414,\n",
              " 'linguistic': 363,\n",
              " 'programming': 500,\n",
              " 'nlp': 415,\n",
              " 'pseudoscientific': 506,\n",
              " 'approach': 73,\n",
              " 'communication': 162,\n",
              " 'personal': 469,\n",
              " 'development': 205,\n",
              " 'psychotherapy': 507,\n",
              " 'created': 175,\n",
              " 'richard': 533,\n",
              " 'bandler': 95,\n",
              " 'john': 339,\n",
              " 'grinder': 290,\n",
              " 'california': 136,\n",
              " 'united': 623,\n",
              " '1970s': 15,\n",
              " 'serving': 548,\n",
              " 'bowl': 126,\n",
              " 'round': 534,\n",
              " 'dish': 212,\n",
              " 'typically': 620,\n",
              " 'prepare': 489,\n",
              " 'serve': 547,\n",
              " 'interior': 330,\n",
              " 'characteristically': 150,\n",
              " 'spherical': 570,\n",
              " 'cap': 141,\n",
              " 'edges': 224,\n",
              " 'bottom': 125,\n",
              " 'forming': 268,\n",
              " 'seamless': 540,\n",
              " 'curve': 182,\n",
              " 'makes': 372,\n",
              " 'bowls': 127,\n",
              " 'especially': 236,\n",
              " 'suited': 584,\n",
              " 'holding': 305,\n",
              " 'liquids': 365,\n",
              " 'contents': 172,\n",
              " 'naturally': 409,\n",
              " 'concentrated': 165,\n",
              " 'center': 149,\n",
              " 'force': 265,\n",
              " 'gravity': 289}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show occurance of vocabulary words each document, with each row corresponding the each documnent\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txNjLK92pa18",
        "outputId": "431f48e1-09cf-456b-d1f2-806fafc279c8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0 ... 1 0 0]\n",
            " [0 1 1 ... 1 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word frequency in each document\n",
        "import numpy as np\n",
        "word_freq = np.sum(X.toarray(), axis=0)\n",
        "\n",
        "word_freq_dict = dict(zip(cv.get_feature_names_out(), word_freq))\n",
        "print(\"\\nWord Frequency:\\n\", word_freq_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-Ur7RbnpBCR",
        "outputId": "0ff77b2f-4fae-4f45-f29a-d45c7b3a6afc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Frequency:\n",
            " {'00': 1, '00kg': 2, '10': 1, '10kg': 3, '12': 4, '15': 1, '1540s': 1, '15kg': 3, '16mm': 1, '1753': 1, '17kg': 3, '180gm': 2, '18kg': 3, '18mm': 2, '1966': 1, '1970s': 1, '1996': 1, '20': 1, '2007': 1, '2021': 1, '2040': 2, '20ft': 3, '20kg': 3, '220': 1, '225': 1, '2400': 2, '25kg': 3, '275': 1, '275gms': 1, '28': 3, '290': 1, '30': 3, '30kg': 3, '320gms': 1, '325gms': 1, '3400': 2, '350': 2, '3kg': 3, '400gm': 2, '400gms': 2, '40ft': 3, '4400': 1, '45': 3, '50': 5, '500': 2, '55': 3, '5500': 1, '5kg': 5, '60': 3, '70': 3, '9kg': 3, 'aai': 6, 'ability': 6, 'about': 2, 'above': 6, 'absorption': 1, 'academic': 1, 'acclaimed': 1, 'according': 1, 'account': 1, 'across': 1, 'actively': 1, 'agro': 6, 'all': 6, 'along': 1, 'also': 5, 'an': 5, 'analyses': 1, 'and': 24, 'animals': 1, 'anushka': 12, 'appearance': 1, 'apples': 1, 'approach': 1, 'apricots': 1, 'april': 3, 'arabia': 1, 'arakta': 1, 'are': 14, 'arils': 2, 'as': 13, 'assimilation': 1, 'assortment': 6, 'attached': 1, 'attractive': 1, 'august': 1, 'availability': 4, 'available': 7, 'avni': 12, 'aware': 1, 'background': 1, 'badlands': 1, 'bag': 3, 'bags': 2, 'bahrain': 1, 'bandler': 1, 'bangladesh': 1, 'banks': 1, 'base': 1, 'based': 1, 'basic': 1, 'be': 3, 'bean': 1, 'beginning': 1, 'beginnings': 1, 'behaviours': 1, 'behind': 1, 'bend': 1, 'benefits': 1, 'berry': 3, 'best': 3, 'between': 1, 'bhagwa': 1, 'big': 1, 'bigger': 1, 'binomial': 1, 'biochemical': 1, 'biological': 1, 'biology': 1, 'biosynthesis': 1, 'black': 2, 'bold': 1, 'botanical': 1, 'botanist': 2, 'botany': 2, 'bottom': 1, 'bowl': 1, 'bowls': 1, 'box': 2, 'branch': 1, 'branches': 1, 'bronco': 1, 'bulk': 2, 'but': 1, 'buyers': 6, 'by': 7, 'california': 1, 'called': 4, 'can': 1, 'canada': 4, 'canadian': 2, 'cap': 1, 'carbohydrates': 1, 'carl': 1, 'carry': 2, 'carton': 4, 'cartons': 4, 'catabolism': 1, 'catalogue': 1, 'center': 1, 'characteristically': 1, 'cherry': 1, 'choosing': 1, 'chutney': 1, 'cis': 1, 'classes': 1, 'client': 1, 'clients': 1, 'collection': 1, 'collections': 1, 'color': 1, 'come': 8, 'communication': 1, 'company': 2, 'comparison': 1, 'concentrated': 1, 'concepts': 1, 'condiments': 1, 'consider': 1, 'contained': 1, 'container': 7, 'content': 1, 'contents': 1, 'corn': 1, 'count': 2, 'created': 1, 'crisp': 1, 'crunchy': 1, 'cucumbers': 1, 'cuisines': 1, 'cultivars': 2, 'cultivate': 1, 'curve': 1, 'customers': 1, 'dark': 2, 'data': 2, 'day': 2, 'dec': 1, 'december': 2, 'deep': 1, 'defined': 1, 'delicious': 1, 'deliver': 1, 'deliveries': 1, 'delivering': 1, 'demand': 1, 'demands': 1, 'describe': 1, 'describing': 1, 'description': 1, 'designed': 1, 'destination': 1, 'details': 4, 'determined': 1, 'developed': 1, 'development': 1, 'develops': 1, 'diamond': 1, 'dictionary': 1, 'diet': 2, 'dietary': 1, 'different': 1, 'dish': 1, 'dishes': 1, 'distance': 1, 'do': 1, 'documents': 1, 'domestic': 1, 'each': 2, 'earliest': 1, 'early': 1, 'eating': 1, 'eats': 1, 'economic': 1, 'edges': 1, 'edible': 1, 'education': 1, 'efforts': 1, 'eggplants': 1, 'either': 1, 'element': 1, 'emerged': 1, 'engaged': 1, 'enhancer': 1, 'enhances': 1, 'entity': 1, 'especially': 1, 'established': 1, 'estimate': 1, 'evolving': 1, 'excretion': 1, 'exporter': 6, 'exporters': 3, 'exporting': 1, 'exportingagro': 1, 'exports': 1, 'extremely': 1, 'facilitated': 1, 'fact': 1, 'fats': 1, 'february': 3, 'feel': 6, 'fertilized': 1, 'fiber': 1, 'field': 1, 'final': 1, 'first': 2, 'five': 1, 'flame': 1, 'flesh': 2, 'flower': 1, 'follow': 4, 'food': 2, 'foods': 2, 'for': 8, 'force': 1, 'ford': 1, 'forerunners': 1, 'forming': 1, 'forms': 1, 'forproviding': 1, 'founded': 1, 'free': 1, 'fresh': 7, 'from': 11, 'fruit': 5, 'fruits': 2, 'garden': 1, 'gardens': 1, 'generation': 1, 'generations': 1, 'glossy': 1, 'gms': 2, 'government': 1, 'grain': 1, 'gram': 1, 'grape': 1, 'grapes': 3, 'grapesflame': 1, 'gravity': 1, 'grinder': 1, 'grouped': 1, 'grow': 1, 'guide': 1, 'guides': 1, 'habits': 1, 'handling': 1, 'harvest': 1, 'have': 7, 'health': 2, 'healthy': 1, 'help': 1, 'herbalism': 1, 'highly': 1, 'hold': 1, 'holding': 1, 'holds': 1, 'hongkong': 1, 'horticultureand': 1, 'huge': 6, 'humans': 2, 'idea': 1, 'identify': 1, 'importance': 1, 'in': 22, 'included': 1, 'includes': 1, 'including': 2, 'income': 1, 'increase': 1, 'india': 2, 'indian': 2, 'indicators': 1, 'industry': 1, 'information': 1, 'ingestion': 1, 'ingredient': 1, 'initiatives': 1, 'instances': 1, 'intake': 1, 'interior': 1, 'international': 12, 'internationalmarket': 1, 'is': 13, 'it': 5, 'items': 1, 'its': 4, 'itself': 6, 'january': 3, 'john': 1, 'july': 1, 'jute': 3, 'keep': 1, 'kernels': 1, 'key': 1, 'kg': 5, 'kilo': 2, 'kinds': 3, 'known': 1, 'land': 1, 'language': 1, 'lanka': 1, 'largely': 1, 'later': 1, 'leading': 1, 'led': 1, 'lexicographical': 1, 'life': 3, 'lifestyles': 1, 'like': 5, 'likeness': 1, 'limiting': 1, 'line': 1, 'linguistic': 1, 'linnaeus': 1, 'liquids': 1, 'list': 1, 'load': 6, 'loading': 1, 'loose': 3, 'macronutrients': 1, 'major': 4, 'makes': 1, 'making': 1, 'malaysia': 2, 'maldives': 1, 'mangos': 1, 'manufacture': 1, 'manufactured': 1, 'many': 1, 'map': 1, 'march': 3, 'market': 6, 'marketed': 1, 'mathematical': 1, 'mauritius': 1, 'maximum': 2, 'meaning': 1, 'means': 1, 'medical': 1, 'medicinal': 1, 'medieval': 1, 'medium': 1, 'melons': 1, 'mesh': 3, 'metric': 1, 'microgram': 1, 'micronutrients': 1, 'milligram': 1, 'minerals': 1, 'minimum': 2, 'mm': 3, 'model': 1, 'monasteries': 1, 'more': 4, 'moreover': 1, 'most': 2, 'mt': 3, 'naming': 1, 'naturally': 1, 'nature': 1, 'needed': 1, 'net': 2, 'nether': 1, 'neuro': 1, 'nlp': 1, 'no': 1, 'nomenclature': 1, 'nov': 1, 'number': 1, 'numbers': 2, 'numerical': 1, 'nutrients': 1, 'nutrition': 3, 'nutritional': 1, 'obtained': 1, 'october': 1, 'odo': 3, 'of': 27, 'offer': 6, 'often': 2, 'ofthe': 1, 'oldest': 1, 'oman': 1, 'on': 2, 'one': 10, 'onion': 3, 'onions': 4, 'only': 1, 'onwards': 1, 'opposed': 1, 'or': 7, 'organism': 3, 'organization': 1, 'organizations': 1, 'originated': 1, 'other': 1, 'our': 1, 'ourselves': 1, 'outer': 1, 'oval': 1, 'ovary': 1, 'over': 1, 'packaging': 7, 'packed': 2, 'packing': 3, 'padua': 1, 'pair': 1, 'palatability': 1, 'pallet': 1, 'pallets': 1, 'participating': 1, 'pea': 1, 'peaches': 1, 'pears': 1, 'people': 1, 'peppers': 1, 'per': 7, 'perishables': 1, 'personal': 1, 'philippines': 1, 'physic': 1, 'physiological': 1, 'phytologist': 1, 'phytology': 1, 'pink': 2, 'plant': 2, 'plants': 2, 'pleasing': 1, 'pleasure': 6, 'plenty': 1, 'pods': 1, 'poisonous': 1, 'pomegranate': 2, 'popular': 1, 'possess': 1, 'practices': 3, 'prehistory': 1, 'premium': 1, 'prepare': 1, 'prepared': 1, 'presenting': 6, 'preservatives': 1, 'primary': 1, 'process': 1, 'processed': 1, 'produced': 1, 'product': 2, 'products': 6, 'professionals': 1, 'programming': 1, 'promote': 1, 'promoting': 1, 'protein': 1, 'proteins': 1, 'proud': 6, 'pseudoscientific': 1, 'psychotherapy': 1, 'publication': 1, 'pumpkins': 1, 'punnets10': 2, 'pure': 1, 'purple': 2, 'purplish': 1, 'python': 1, 'qatar': 1, 'quality': 7, 'quantities': 1, 'read': 3, 'recognizing': 6, 'recommends': 1, 'red': 7, 'refer': 3, 'relationship': 1, 'relevant': 1, 'remains': 1, 'renowned': 6, 'reported': 1, 'reproduce': 1, 'reputed': 1, 'requested': 1, 'requirement': 1, 'requirements': 1, 'richard': 1, 'round': 1, 'rugged': 1, 'russia': 1, 'saudi': 1, 'science': 3, 'scientist': 1, 'seamless': 1, 'second': 1, 'seed': 1, 'seedless': 3, 'seeds': 1, 'semantic': 1, 'september': 1, 'serve': 1, 'serving': 1, 'set': 1, 'seven': 1, 'shades': 1, 'shaped': 2, 'sharad': 1, 'shelf': 1, 'similarity': 1, 'singapore': 2, 'single': 1, 'six': 1, 'sixth': 1, 'size': 4, 'sized': 1, 'sizes': 3, 'skin': 4, 'so': 1, 'soft': 2, 'sold': 1, 'sometimes': 2, 'specialises': 1, 'species': 1, 'spherical': 1, 'sport': 1, 'spring': 1, 'squash': 1, 'sri': 1, 'standard': 6, 'states': 2, 'statistics': 1, 'store': 1, 'strength': 1, 'strong': 1, 'studies': 1, 'study': 1, 'substances': 1, 'suited': 1, 'sum': 1, 'summer': 1, 'suppliers': 9, 'suppling': 1, 'supplying': 3, 'support': 1, 'supporting': 1, 'survive': 1, 'suv': 1, 'sweet': 3, 'system': 1, 'table': 1, 'takes': 6, 'tart': 1, 'taste': 4, 'tax': 1, 'taxonomy': 1, 'terms': 1, 'that': 7, 'the': 24, 'thecustomers': 1, 'their': 2, 'these': 4, 'they': 1, 'this': 8, 'thomson': 1, 'thoroughly': 1, 'those': 1, 'through': 1, 'till': 1, 'tinged': 2, 'to': 17, 'tomatoes': 2, 'tools': 2, 'types': 1, 'typically': 1, 'uae': 1, 'uk': 1, 'united': 1, 'units': 1, 'universities': 1, 'unlike': 1, 'unordered': 1, 'us': 6, 'use': 2, 'used': 5, 'useful': 1, 'uses': 1, 'usually': 1, 'utility': 1, 'value': 1, 'values': 1, 'varieties': 1, 'variety': 2, 'vary': 1, 'vegetables': 1, 'vehicles': 1, 'versions': 1, 'vietnam': 1, 'vitamins': 1, 'was': 2, 'water': 1, 'we': 11, 'website': 1, 'weight': 2, 'welcome': 1, 'were': 2, 'when': 6, 'where': 1, 'which': 10, 'while': 3, 'whilst': 1, 'white': 3, 'who': 1, 'whole': 1, 'wide': 6, 'widely': 2, 'wildtrak': 1, 'will': 1, 'with': 15, 'world': 6, 'wt': 2, 'year': 1, 'years': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(word_freq_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNhmTpY4tg4o",
        "outputId": "08ea6a94-c502-449c-cc9f-10cabf91f267"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_top_similarity(query, top_n=5):\n",
        "  vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
        "  vectors = vectorizer.fit_transform([query] + documents)\n",
        "\n",
        "  # Calculate similarity scores\n",
        "  # Compare first vector(vectors[0:1]) with all other vectors in vectors\n",
        "  # linear_kernel: compute dot product of vectors, this is the cosine similarity when the vectors are normalized\n",
        "  cosine_similarities = linear_kernel(vectors[0:1], vectors).flatten()\n",
        "\n",
        "  # Output the similarity scores for the top 5 documents\n",
        "  top_indices = cosine_similarities.argsort()[-2:-7:-1]\n",
        "  print(\"Query:\", query)\n",
        "  print(\"Top 5 similar documents:\")\n",
        "  for idx in top_indices:\n",
        "      print(f\"Document {idx-1}: \\t{documents[idx-1][:80]}, \\tsimilarity score {round(cosine_similarities[idx], 2)}\")\n"
      ],
      "metadata": {
        "id": "Fe3KcfoKHSp_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = ['fruits', 'vegetables', 'healthy foods in Canada']\n",
        "for query in queries:\n",
        "  find_top_similarity(query)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40DqMMM2Iu5a",
        "outputId": "90bf5a50-c095-4700-bc83-bcd64e3e5404"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: fruits\n",
            "Top 5 similar documents:\n",
            "Document 6: \tTo a botanist, a fruit is an entity that develops from the fertilized ovary of a, \tsimilarity score 0.16\n",
            "Document 10: \tCanada's Food Guide is a nutrition guide produced by Health Canada to promote He, \tsimilarity score 0.07\n",
            "Document 14: \tBerry Size: 18mm and above Packaging Packing Size: 4.5 kg loose in carry bags 8., \tsimilarity score 0.0\n",
            "Document 0: \tFresh Pomegranate from Anushka Avni International Bhagwa is a premium Pomegranat, \tsimilarity score 0.0\n",
            "Document 1: \tFresh Pomegranate Arakta from Anushka Avni International This Pomegranate are bi, \tsimilarity score 0.0\n",
            "\n",
            "Query: vegetables\n",
            "Top 5 similar documents:\n",
            "Document 10: \tCanada's Food Guide is a nutrition guide produced by Health Canada to promote He, \tsimilarity score 0.08\n",
            "Document 14: \tBerry Size: 18mm and above Packaging Packing Size: 4.5 kg loose in carry bags 8., \tsimilarity score 0.0\n",
            "Document 0: \tFresh Pomegranate from Anushka Avni International Bhagwa is a premium Pomegranat, \tsimilarity score 0.0\n",
            "Document 1: \tFresh Pomegranate Arakta from Anushka Avni International This Pomegranate are bi, \tsimilarity score 0.0\n",
            "Document 2: \tAbout Us Anushka Avni International (AAI) takes pleasure in presenting itself as, \tsimilarity score 0.0\n",
            "\n",
            "Query: healthy foods in Canada\n",
            "Top 5 similar documents:\n",
            "Document 10: \tCanada's Food Guide is a nutrition guide produced by Health Canada to promote He, \tsimilarity score 0.6\n",
            "Document 9: \tIn nutrition, the diet of an organism is the sum of foods it eats, which is larg, \tsimilarity score 0.3\n",
            "Document 12: \tCanadian Industry Statistics (CIS) analyses industry data on many economic indic, \tsimilarity score 0.1\n",
            "Document 11: \tUK, Nether Land, Russia, Canada, HongKong, Malaysia, Singapore, Oman, UAE, Qatar, \tsimilarity score 0.09\n",
            "Document 28: \tThe Ford Bronco is a model line of sport utility vehicles manufactured and marke, \tsimilarity score 0.07\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "hM5Yp3TvJ70D",
        "outputId": "0399efac-b95a-4d40-b1a2-400936b97e68"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Canada's Food Guide is a nutrition guide produced by Health Canada to promote Healthy behaviours and habits, and lifestyles in Canada - this is to increase the number of healthy people in Canada. In 2007, it was reported to be the second most requested Canadian government publication, behind the Income Tax Forms. The Health Canada website states: Food guides are basic education tools that are designed to help people follow a healthy diet. The Guide recommends eating a variety of healthy foods each day including plenty of vegetables and fruits, protein foods, and whole grain foods. It recommends choosing protein foods that come from plants more often. It also recommends limiting highly processed foods.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Documents[10] came up in top 5 of all three queries."
      ],
      "metadata": {
        "id": "pwo3yr-cKIVA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqrN27J9mZwb"
      },
      "source": [
        "## Repeat the same task after some preprocessing\n",
        "\n",
        "Use a minimum of 2 different text cleaning/standardization techniques (e.g. lemmatization, removing punctuation, etc)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhnwdZpFOl9O",
        "outputId": "783004be-77f2-4d66-b9c4-0704923d16ef"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSsdfQb9g46z"
      },
      "source": [
        "# e.g. you can use a lemmatizer to reduce words down to their\n",
        "# simplest 'lemma' (helpful when dealing with plurals)\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"\n",
        "    Convert Treebank POS tags to WordNet POS tags\n",
        "    The Penn Treebank tagset uses different tags (eg, NN for noun)\n",
        "    than WordNet (e.g., N for noun)\n",
        "    \"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # default to noun if POS tag is not found\n"
      ],
      "metadata": {
        "id": "NQ5LrFHxPfEJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_document(document):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = word_tokenize(document)\n",
        "\n",
        "    # Remove punctuation and convert to lowercase\n",
        "    words = [word.lower() for word in words if word.isalnum()]\n",
        "\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
        "    return ' '.join(lemmatized_words)"
      ],
      "metadata": {
        "id": "7iCZN9lYPW6V"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing\n",
        "lemmatized_documents = [lemmatize_document(doc) for doc in documents]"
      ],
      "metadata": {
        "id": "ybARxNm6r1cD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_documents[0][:80]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "nYF6fsissH7y",
        "outputId": "84165e05-3eb5-4d4a-b07b-be179b5fd43d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fresh pomegranate from anushka avni international bhagwa be a premium pomegranat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Frequency"
      ],
      "metadata": {
        "id": "kkDxzK8UsIdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use binary=True for sentences, not words\n",
        "cv = CountVectorizer(binary=True)\n",
        "X = cv.fit_transform(lemmatized_documents)"
      ],
      "metadata": {
        "id": "LQGGni76sH_U"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUuFYclnr-aq",
        "outputId": "f85e02ab-bdd0-4ca7-dbf3-08033d0ed0d8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fresh': 238,\n",
              " 'pomegranate': 425,\n",
              " 'from': 239,\n",
              " 'anushka': 56,\n",
              " 'avni': 73,\n",
              " 'international': 287,\n",
              " 'bhagwa': 95,\n",
              " 'be': 84,\n",
              " 'premium': 430,\n",
              " 'variety': 563,\n",
              " 'india': 276,\n",
              " 'the': 534,\n",
              " 'deep': 165,\n",
              " 'red': 459,\n",
              " 'arils': 65,\n",
              " 'pleasing': 420,\n",
              " 'but': 113,\n",
              " 'rugged': 472,\n",
              " 'skin': 497,\n",
              " 'enhance': 202,\n",
              " 'appearance': 57,\n",
              " 'whilst': 579,\n",
              " 'promote': 440,\n",
              " 'shelf': 490,\n",
              " 'life': 310,\n",
              " 'of': 375,\n",
              " 'fruit': 240,\n",
              " 'widely': 584,\n",
              " 'know': 302,\n",
              " 'for': 230,\n",
              " 'it': 289,\n",
              " 'soft': 499,\n",
              " 'seed': 479,\n",
              " 'dark': 160,\n",
              " 'color': 137,\n",
              " 'and': 54,\n",
              " 'extremely': 213,\n",
              " 'delicious': 167,\n",
              " 'package': 398,\n",
              " 'net': 360,\n",
              " 'weight': 573,\n",
              " 'box': 109,\n",
              " 'detail': 175,\n",
              " 'minimum': 348,\n",
              " '180gm': 9,\n",
              " 'maximum': 335,\n",
              " '400gm': 28,\n",
              " 'aril': 64,\n",
              " 'cherry': 130,\n",
              " 'taste': 529,\n",
              " 'sweet': 525,\n",
              " 'count': 150,\n",
              " 'carton': 125,\n",
              " 'kg': 299,\n",
              " 'wt': 589,\n",
              " 'number': 367,\n",
              " 'pack': 397,\n",
              " 'per': 410,\n",
              " 'arakta': 63,\n",
              " 'this': 539,\n",
              " 'big': 96,\n",
              " 'in': 272,\n",
              " 'size': 496,\n",
              " 'with': 587,\n",
              " 'bold': 103,\n",
              " 'also': 51,\n",
              " 'possess': 427,\n",
              " 'glossy': 243,\n",
              " 'attractive': 69,\n",
              " 'packaging': 399,\n",
              " '10': 0,\n",
              " '12': 2,\n",
              " '15': 3,\n",
              " 'load': 320,\n",
              " 'ability': 38,\n",
              " '4400': 30,\n",
              " 'container': 147,\n",
              " '20': 15,\n",
              " 'pallet': 403,\n",
              " '220': 21,\n",
              " '5500': 33,\n",
              " 'no': 364,\n",
              " 'availability': 71,\n",
              " 'january': 292,\n",
              " 'february': 217,\n",
              " 'march': 331,\n",
              " 'april': 61,\n",
              " 'july': 294,\n",
              " 'august': 70,\n",
              " 'september': 483,\n",
              " 'october': 373,\n",
              " 'nov': 366,\n",
              " 'dec': 163,\n",
              " 'read': 456,\n",
              " 'more': 352,\n",
              " 'about': 39,\n",
              " 'aai': 37,\n",
              " 'take': 528,\n",
              " 'pleasure': 421,\n",
              " 'present': 432,\n",
              " 'itself': 291,\n",
              " 'one': 382,\n",
              " 'renowned': 464,\n",
              " 'supplier': 520,\n",
              " 'exporter': 211,\n",
              " 'we': 571,\n",
              " 'have': 258,\n",
              " 'huge': 267,\n",
              " 'assortment': 67,\n",
              " 'agro': 48,\n",
              " 'product': 437,\n",
              " 'available': 72,\n",
              " 'feel': 218,\n",
              " 'proud': 443,\n",
              " 'when': 575,\n",
              " 'buyer': 114,\n",
              " 'come': 138,\n",
              " 'to': 546,\n",
              " 'recognize': 457,\n",
              " 'standard': 508,\n",
              " 'quality': 454,\n",
              " 'which': 577,\n",
              " 'offer': 376,\n",
              " 'world': 588,\n",
              " 'wide': 583,\n",
              " 'market': 332,\n",
              " 'follow': 228,\n",
              " 'best': 93,\n",
              " 'practice': 428,\n",
              " 'while': 578,\n",
              " 'white': 580,\n",
              " 'onion': 383,\n",
              " 'acclaim': 43,\n",
              " 'health': 259,\n",
              " 'benefit': 91,\n",
              " 'use': 558,\n",
              " 'different': 183,\n",
              " 'cuisine': 155,\n",
              " 'major': 323,\n",
              " 'all': 49,\n",
              " 'kind': 301,\n",
              " 'like': 312,\n",
              " 'pink': 418,\n",
              " 'mm': 349,\n",
              " '50': 31,\n",
              " 'above': 40,\n",
              " '60': 35,\n",
              " '3kg': 27,\n",
              " '5kg': 34,\n",
              " '9kg': 36,\n",
              " '10kg': 1,\n",
              " '15kg': 5,\n",
              " '17kg': 8,\n",
              " '18kg': 10,\n",
              " '20kg': 20,\n",
              " '25kg': 23,\n",
              " '30kg': 25,\n",
              " 'mesh': 342,\n",
              " 'bag': 77,\n",
              " 'or': 387,\n",
              " 'jute': 295,\n",
              " 'mt': 355,\n",
              " '20ft': 19,\n",
              " 'odo': 374,\n",
              " '28': 24,\n",
              " '40ft': 29,\n",
              " 'refer': 460,\n",
              " 'botanist': 105,\n",
              " 'an': 52,\n",
              " 'entity': 204,\n",
              " 'that': 533,\n",
              " 'develop': 177,\n",
              " 'fertilized': 219,\n",
              " 'ovary': 395,\n",
              " 'flower': 227,\n",
              " 'mean': 336,\n",
              " 'tomato': 547,\n",
              " 'squash': 506,\n",
              " 'pumpkin': 447,\n",
              " 'cucumber': 154,\n",
              " 'pepper': 409,\n",
              " 'eggplant': 197,\n",
              " 'corn': 149,\n",
              " 'kernel': 297,\n",
              " 'bean': 85,\n",
              " 'pea': 405,\n",
              " 'pod': 423,\n",
              " 'so': 498,\n",
              " 'apple': 58,\n",
              " 'pear': 407,\n",
              " 'peach': 406,\n",
              " 'apricots': 60,\n",
              " 'melon': 341,\n",
              " 'mango': 327,\n",
              " 'nutrition': 370,\n",
              " 'biochemical': 98,\n",
              " 'physiological': 415,\n",
              " 'process': 435,\n",
              " 'by': 115,\n",
              " 'organism': 388,\n",
              " 'intake': 285,\n",
              " 'support': 522,\n",
              " 'include': 273,\n",
              " 'ingestion': 281,\n",
              " 'absorption': 41,\n",
              " 'assimilation': 66,\n",
              " 'biosynthesis': 101,\n",
              " 'catabolism': 126,\n",
              " 'excretion': 209,\n",
              " 'science': 475,\n",
              " 'study': 514,\n",
              " 'call': 117,\n",
              " 'nutritional': 371,\n",
              " 'nutrient': 369,\n",
              " 'substance': 515,\n",
              " 'survive': 523,\n",
              " 'grow': 253,\n",
              " 'reproduce': 466,\n",
              " 'seven': 486,\n",
              " 'class': 134,\n",
              " 'relevant': 462,\n",
              " 'animal': 55,\n",
              " 'human': 268,\n",
              " 'carbohydrates': 122,\n",
              " 'dietary': 182,\n",
              " 'fiber': 220,\n",
              " 'fat': 216,\n",
              " 'proteins': 442,\n",
              " 'mineral': 347,\n",
              " 'vitamin': 569,\n",
              " 'water': 570,\n",
              " 'can': 118,\n",
              " 'group': 252,\n",
              " 'either': 198,\n",
              " 'macronutrients': 322,\n",
              " 'protein': 441,\n",
              " 'need': 359,\n",
              " 'gram': 247,\n",
              " 'quantity': 455,\n",
              " 'micronutrient': 345,\n",
              " 'milligram': 346,\n",
              " 'microgram': 344,\n",
              " 'diet': 181,\n",
              " 'sum': 517,\n",
              " 'food': 229,\n",
              " 'eat': 191,\n",
              " 'largely': 306,\n",
              " 'determine': 176,\n",
              " 'palatability': 402,\n",
              " 'canada': 119,\n",
              " 'guide': 254,\n",
              " 'produce': 436,\n",
              " 'healthy': 260,\n",
              " 'behaviour': 88,\n",
              " 'habit': 255,\n",
              " 'lifestyle': 311,\n",
              " 'increase': 275,\n",
              " 'people': 408,\n",
              " '2007': 16,\n",
              " 'report': 465,\n",
              " 'second': 478,\n",
              " 'most': 354,\n",
              " 'requested': 468,\n",
              " 'canadian': 120,\n",
              " 'government': 245,\n",
              " 'publication': 446,\n",
              " 'behind': 89,\n",
              " 'income': 274,\n",
              " 'tax': 530,\n",
              " 'form': 234,\n",
              " 'website': 572,\n",
              " 'state': 509,\n",
              " 'basic': 83,\n",
              " 'education': 195,\n",
              " 'tool': 548,\n",
              " 'design': 173,\n",
              " 'help': 261,\n",
              " 'recommend': 458,\n",
              " 'each': 189,\n",
              " 'day': 162,\n",
              " 'plenty': 422,\n",
              " 'vegetable': 565,\n",
              " 'whole': 582,\n",
              " 'grain': 246,\n",
              " 'choose': 131,\n",
              " 'plant': 419,\n",
              " 'often': 377,\n",
              " 'limit': 314,\n",
              " 'highly': 263,\n",
              " 'uk': 552,\n",
              " 'nether': 361,\n",
              " 'land': 303,\n",
              " 'russia': 473,\n",
              " 'hongkong': 265,\n",
              " 'malaysia': 325,\n",
              " 'singapore': 492,\n",
              " 'oman': 380,\n",
              " 'uae': 551,\n",
              " 'qatar': 453,\n",
              " 'bahrain': 78,\n",
              " 'saudi': 474,\n",
              " 'arabia': 62,\n",
              " 'mauritius': 334,\n",
              " 'maldives': 326,\n",
              " 'sri': 507,\n",
              " 'lanka': 305,\n",
              " 'bangladesh': 80,\n",
              " 'industry': 279,\n",
              " 'statistic': 510,\n",
              " 'cis': 133,\n",
              " 'analysis': 53,\n",
              " 'data': 161,\n",
              " 'on': 381,\n",
              " 'many': 329,\n",
              " 'economic': 192,\n",
              " 'indicator': 278,\n",
              " 'sometimes': 500,\n",
              " 'purple': 450,\n",
              " 'cultivar': 156,\n",
              " 'purplish': 451,\n",
              " 'flesh': 226,\n",
              " 'ting': 545,\n",
              " 'indian': 277,\n",
              " 'berry': 92,\n",
              " '18mm': 11,\n",
              " 'loose': 321,\n",
              " 'carry': 124,\n",
              " 'punnets10': 448,\n",
              " '500': 32,\n",
              " 'gms': 244,\n",
              " '2400': 22,\n",
              " 'kilo': 300,\n",
              " '3400': 26,\n",
              " '2040': 18,\n",
              " 'december': 164,\n",
              " 'summer': 518,\n",
              " 'supply': 521,\n",
              " 'exportingagro': 212,\n",
              " 'thoroughly': 541,\n",
              " 'consider': 145,\n",
              " 'demand': 170,\n",
              " 'requirement': 469,\n",
              " 'ofthe': 378,\n",
              " 'client': 135,\n",
              " 'till': 544,\n",
              " 'final': 222,\n",
              " 'destination': 174,\n",
              " 'company': 140,\n",
              " 'professional': 438,\n",
              " 'aware': 74,\n",
              " 'fact': 215,\n",
              " 'thecustomers': 535,\n",
              " 'keep': 296,\n",
              " 'evolve': 208,\n",
              " 'do': 186,\n",
              " 'strong': 513,\n",
              " 'background': 75,\n",
              " 'horticultureand': 266,\n",
              " 'harvest': 257,\n",
              " 'handling': 256,\n",
              " 'perishable': 411,\n",
              " 'initiative': 283,\n",
              " 'deliver': 168,\n",
              " 'moreover': 353,\n",
              " 'establish': 206,\n",
              " 'ourselves': 393,\n",
              " 'forproviding': 235,\n",
              " 'across': 46,\n",
              " 'domestic': 188,\n",
              " 'internationalmarket': 288,\n",
              " 'lead': 308,\n",
              " 'organization': 389,\n",
              " 'engage': 201,\n",
              " 'our': 392,\n",
              " 'customer': 159,\n",
              " 'manufacture': 328,\n",
              " 'bulk': 112,\n",
              " 'philippine': 413,\n",
              " 'vietnam': 568,\n",
              " 'emerge': 200,\n",
              " 'reputed': 467,\n",
              " 'actively': 47,\n",
              " 'participate': 404,\n",
              " 'export': 210,\n",
              " 'supple': 519,\n",
              " 'enhancer': 203,\n",
              " 'free': 237,\n",
              " 'preservative': 433,\n",
              " 'pure': 449,\n",
              " 'useful': 559,\n",
              " 'chutney': 132,\n",
              " 'welcome': 574,\n",
              " 'thomson': 540,\n",
              " 'seedless': 480,\n",
              " 'these': 537,\n",
              " 'grape': 248,\n",
              " 'crunchy': 153,\n",
              " 'account': 45,\n",
              " 'table': 527,\n",
              " '16mm': 6,\n",
              " 'black': 102,\n",
              " 'sharad': 489,\n",
              " 'crisp': 152,\n",
              " 'vary': 564,\n",
              " 'shade': 487,\n",
              " 'usually': 560,\n",
              " 'flame': 225,\n",
              " 'grapesflame': 249,\n",
              " 'popular': 426,\n",
              " 'along': 50,\n",
              " 'dictionary': 180,\n",
              " 'python': 452,\n",
              " 'unordered': 557,\n",
              " 'collection': 136,\n",
              " 'value': 562,\n",
              " 'store': 511,\n",
              " 'map': 330,\n",
              " 'unlike': 556,\n",
              " 'other': 391,\n",
              " 'type': 549,\n",
              " 'hold': 264,\n",
              " 'only': 384,\n",
              " 'single': 493,\n",
              " 'element': 199,\n",
              " 'key': 298,\n",
              " 'pair': 401,\n",
              " 'botany': 106,\n",
              " 'originate': 390,\n",
              " 'prehistory': 429,\n",
              " 'herbalism': 262,\n",
              " 'effort': 196,\n",
              " 'early': 190,\n",
              " 'identify': 270,\n",
              " 'later': 307,\n",
              " 'cultivate': 157,\n",
              " 'edible': 194,\n",
              " 'medicinal': 339,\n",
              " 'poisonous': 424,\n",
              " 'make': 324,\n",
              " 'old': 379,\n",
              " 'branch': 110,\n",
              " 'medieval': 340,\n",
              " 'physic': 414,\n",
              " 'garden': 241,\n",
              " 'attach': 68,\n",
              " 'monastery': 351,\n",
              " 'contain': 146,\n",
              " 'medical': 338,\n",
              " 'importance': 271,\n",
              " 'they': 538,\n",
              " 'forerunner': 233,\n",
              " 'first': 223,\n",
              " 'botanical': 104,\n",
              " 'university': 555,\n",
              " 'found': 236,\n",
              " '1540s': 4,\n",
              " 'onwards': 385,\n",
              " 'padua': 400,\n",
              " 'facilitate': 214,\n",
              " 'academic': 42,\n",
              " 'catalogue': 127,\n",
              " 'describe': 171,\n",
              " 'their': 536,\n",
              " 'beginning': 87,\n",
              " 'taxonomy': 531,\n",
              " '1753': 7,\n",
              " 'binomial': 97,\n",
              " 'system': 526,\n",
              " 'nomenclature': 365,\n",
              " 'carl': 123,\n",
              " 'linnaeus': 317,\n",
              " 'remain': 463,\n",
              " 'naming': 356,\n",
              " 'biological': 99,\n",
              " 'specie': 502,\n",
              " 'semantic': 482,\n",
              " 'similarity': 491,\n",
              " 'metric': 343,\n",
              " 'define': 166,\n",
              " 'over': 396,\n",
              " 'set': 485,\n",
              " 'document': 187,\n",
              " 'term': 532,\n",
              " 'where': 576,\n",
              " 'idea': 269,\n",
              " 'distance': 185,\n",
              " 'between': 94,\n",
              " 'item': 290,\n",
              " 'base': 82,\n",
              " 'likeness': 313,\n",
              " 'meaning': 337,\n",
              " 'content': 148,\n",
              " 'oppose': 386,\n",
              " 'lexicographical': 309,\n",
              " 'mathematical': 333,\n",
              " 'estimate': 207,\n",
              " 'strength': 512,\n",
              " 'relationship': 461,\n",
              " 'unit': 553,\n",
              " 'language': 304,\n",
              " 'concept': 143,\n",
              " 'instance': 284,\n",
              " 'through': 543,\n",
              " 'numerical': 368,\n",
              " 'description': 172,\n",
              " 'obtain': 372,\n",
              " 'accord': 44,\n",
              " 'comparison': 141,\n",
              " 'information': 280,\n",
              " 'nature': 358,\n",
              " 'biology': 100,\n",
              " 'phytology': 417,\n",
              " 'scientist': 476,\n",
              " 'phytologist': 416,\n",
              " 'who': 581,\n",
              " 'specialise': 501,\n",
              " 'field': 221,\n",
              " 'ford': 232,\n",
              " 'bronco': 111,\n",
              " 'model': 350,\n",
              " 'line': 315,\n",
              " 'sport': 504,\n",
              " 'utility': 561,\n",
              " 'vehicle': 566,\n",
              " 'suv': 524,\n",
              " 'five': 224,\n",
              " 'generation': 242,\n",
              " 'sell': 481,\n",
              " '1966': 12,\n",
              " '1996': 14,\n",
              " 'year': 590,\n",
              " 'sixth': 495,\n",
              " '2021': 17,\n",
              " 'will': 586,\n",
              " 'delivery': 169,\n",
              " 'begin': 86,\n",
              " 'spring': 505,\n",
              " 'six': 494,\n",
              " 'version': 567,\n",
              " 'bend': 90,\n",
              " 'diamond': 179,\n",
              " 'outer': 394,\n",
              " 'bank': 81,\n",
              " 'wildtrak': 585,\n",
              " 'badlands': 76,\n",
              " 'dish': 184,\n",
              " 'those': 542,\n",
              " 'primary': 434,\n",
              " 'ingredient': 282,\n",
              " 'condiment': 144,\n",
              " 'prepare': 431,\n",
              " 'list': 319,\n",
              " 'neuro': 362,\n",
              " 'linguistic': 316,\n",
              " 'programming': 439,\n",
              " 'nlp': 363,\n",
              " 'pseudoscientific': 444,\n",
              " 'approach': 59,\n",
              " 'communication': 139,\n",
              " 'personal': 412,\n",
              " 'development': 178,\n",
              " 'psychotherapy': 445,\n",
              " 'create': 151,\n",
              " 'richard': 470,\n",
              " 'bandler': 79,\n",
              " 'john': 293,\n",
              " 'grinder': 251,\n",
              " 'california': 116,\n",
              " 'united': 554,\n",
              " '1970s': 13,\n",
              " 'serve': 484,\n",
              " 'bowl': 108,\n",
              " 'round': 471,\n",
              " 'typically': 550,\n",
              " 'interior': 286,\n",
              " 'characteristically': 129,\n",
              " 'shape': 488,\n",
              " 'spherical': 503,\n",
              " 'cap': 121,\n",
              " 'edge': 193,\n",
              " 'bottom': 107,\n",
              " 'seamless': 477,\n",
              " 'curve': 158,\n",
              " 'especially': 205,\n",
              " 'suit': 516,\n",
              " 'liquid': 318,\n",
              " 'naturally': 357,\n",
              " 'concentrate': 142,\n",
              " 'center': 128,\n",
              " 'force': 231,\n",
              " 'gravity': 250}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show occurance of vocabulary words each document, with each row corresponding the each lemmatized_documents\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcDb61xsstQP",
        "outputId": "426cd9cf-2a24-4354-fcf7-072ab38e16f6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 1 0]\n",
            " [1 0 1 ... 0 1 0]\n",
            " [0 0 0 ... 1 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq = np.sum(X.toarray(), axis=0)\n",
        "word_freq_dict = dict(zip(cv.get_feature_names_out(), word_freq))\n",
        "print(\"\\nWord Frequency:\\n\", word_freq_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4a6pY67stLy",
        "outputId": "68b0c967-66ea-494f-c72a-637395d62021"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Frequency:\n",
            " {'10': 1, '10kg': 3, '12': 1, '15': 1, '1540s': 1, '15kg': 3, '16mm': 1, '1753': 1, '17kg': 3, '180gm': 2, '18kg': 3, '18mm': 2, '1966': 1, '1970s': 1, '1996': 1, '20': 1, '2007': 1, '2021': 1, '2040': 2, '20ft': 3, '20kg': 3, '220': 1, '2400': 2, '25kg': 3, '28': 3, '30kg': 3, '3400': 2, '3kg': 3, '400gm': 2, '40ft': 3, '4400': 1, '50': 3, '500': 2, '5500': 1, '5kg': 3, '60': 3, '9kg': 3, 'aai': 6, 'ability': 6, 'about': 2, 'above': 6, 'absorption': 1, 'academic': 1, 'acclaim': 1, 'accord': 1, 'account': 1, 'across': 1, 'actively': 1, 'agro': 6, 'all': 6, 'along': 1, 'also': 5, 'an': 5, 'analysis': 1, 'and': 24, 'animal': 1, 'anushka': 12, 'appearance': 1, 'apple': 1, 'approach': 1, 'apricots': 1, 'april': 3, 'arabia': 1, 'arakta': 1, 'aril': 2, 'arils': 1, 'assimilation': 1, 'assortment': 6, 'attach': 1, 'attractive': 1, 'august': 1, 'availability': 4, 'available': 7, 'avni': 12, 'aware': 1, 'background': 1, 'badlands': 1, 'bag': 5, 'bahrain': 1, 'bandler': 1, 'bangladesh': 1, 'bank': 1, 'base': 2, 'basic': 1, 'be': 23, 'bean': 1, 'begin': 1, 'beginning': 1, 'behaviour': 1, 'behind': 1, 'bend': 1, 'benefit': 1, 'berry': 3, 'best': 3, 'between': 1, 'bhagwa': 1, 'big': 2, 'binomial': 1, 'biochemical': 1, 'biological': 1, 'biology': 1, 'biosynthesis': 1, 'black': 2, 'bold': 1, 'botanical': 1, 'botanist': 2, 'botany': 2, 'bottom': 1, 'bowl': 1, 'box': 2, 'branch': 2, 'bronco': 1, 'bulk': 2, 'but': 1, 'buyer': 6, 'by': 7, 'california': 1, 'call': 4, 'can': 1, 'canada': 4, 'canadian': 2, 'cap': 1, 'carbohydrates': 1, 'carl': 1, 'carry': 2, 'carton': 5, 'catabolism': 1, 'catalogue': 1, 'center': 1, 'characteristically': 1, 'cherry': 1, 'choose': 1, 'chutney': 1, 'cis': 1, 'class': 1, 'client': 2, 'collection': 2, 'color': 1, 'come': 8, 'communication': 1, 'company': 2, 'comparison': 1, 'concentrate': 1, 'concept': 1, 'condiment': 1, 'consider': 1, 'contain': 1, 'container': 7, 'content': 2, 'corn': 1, 'count': 2, 'create': 1, 'crisp': 1, 'crunchy': 1, 'cucumber': 1, 'cuisine': 1, 'cultivar': 2, 'cultivate': 1, 'curve': 1, 'customer': 1, 'dark': 2, 'data': 2, 'day': 2, 'dec': 1, 'december': 2, 'deep': 1, 'define': 1, 'delicious': 1, 'deliver': 2, 'delivery': 1, 'demand': 1, 'describe': 2, 'description': 1, 'design': 1, 'destination': 1, 'detail': 4, 'determine': 1, 'develop': 2, 'development': 1, 'diamond': 1, 'dictionary': 1, 'diet': 2, 'dietary': 1, 'different': 1, 'dish': 2, 'distance': 1, 'do': 1, 'document': 1, 'domestic': 1, 'each': 2, 'early': 1, 'eat': 2, 'economic': 1, 'edge': 1, 'edible': 1, 'education': 1, 'effort': 1, 'eggplant': 1, 'either': 1, 'element': 1, 'emerge': 1, 'engage': 1, 'enhance': 1, 'enhancer': 1, 'entity': 1, 'especially': 1, 'establish': 1, 'estimate': 1, 'evolve': 1, 'excretion': 1, 'export': 2, 'exporter': 9, 'exportingagro': 1, 'extremely': 1, 'facilitate': 1, 'fact': 1, 'fat': 1, 'february': 3, 'feel': 6, 'fertilized': 1, 'fiber': 1, 'field': 1, 'final': 1, 'first': 2, 'five': 1, 'flame': 1, 'flesh': 2, 'flower': 1, 'follow': 4, 'food': 3, 'for': 8, 'force': 1, 'ford': 1, 'forerunner': 1, 'form': 2, 'forproviding': 1, 'found': 1, 'free': 1, 'fresh': 7, 'from': 11, 'fruit': 6, 'garden': 1, 'generation': 1, 'glossy': 1, 'gms': 2, 'government': 1, 'grain': 1, 'gram': 1, 'grape': 3, 'grapesflame': 1, 'gravity': 1, 'grinder': 1, 'group': 1, 'grow': 1, 'guide': 1, 'habit': 1, 'handling': 1, 'harvest': 1, 'have': 7, 'health': 2, 'healthy': 1, 'help': 1, 'herbalism': 1, 'highly': 1, 'hold': 2, 'hongkong': 1, 'horticultureand': 1, 'huge': 6, 'human': 2, 'idea': 1, 'identify': 1, 'importance': 1, 'in': 22, 'include': 4, 'income': 1, 'increase': 1, 'india': 2, 'indian': 2, 'indicator': 1, 'industry': 1, 'information': 1, 'ingestion': 1, 'ingredient': 1, 'initiative': 1, 'instance': 1, 'intake': 1, 'interior': 1, 'international': 12, 'internationalmarket': 1, 'it': 8, 'item': 1, 'itself': 6, 'january': 3, 'john': 1, 'july': 1, 'jute': 3, 'keep': 1, 'kernel': 1, 'key': 1, 'kg': 5, 'kilo': 2, 'kind': 3, 'know': 1, 'land': 1, 'language': 1, 'lanka': 1, 'largely': 1, 'later': 1, 'lead': 2, 'lexicographical': 1, 'life': 3, 'lifestyle': 1, 'like': 5, 'likeness': 1, 'limit': 1, 'line': 1, 'linguistic': 1, 'linnaeus': 1, 'liquid': 1, 'list': 1, 'load': 6, 'loose': 3, 'macronutrients': 1, 'major': 4, 'make': 2, 'malaysia': 2, 'maldives': 1, 'mango': 1, 'manufacture': 2, 'many': 1, 'map': 1, 'march': 3, 'market': 7, 'mathematical': 1, 'mauritius': 1, 'maximum': 2, 'mean': 1, 'meaning': 1, 'medical': 1, 'medicinal': 1, 'medieval': 1, 'melon': 1, 'mesh': 3, 'metric': 1, 'microgram': 1, 'micronutrient': 1, 'milligram': 1, 'mineral': 1, 'minimum': 2, 'mm': 3, 'model': 1, 'monastery': 1, 'more': 4, 'moreover': 1, 'most': 2, 'mt': 3, 'naming': 1, 'naturally': 1, 'nature': 1, 'need': 1, 'net': 2, 'nether': 1, 'neuro': 1, 'nlp': 1, 'no': 1, 'nomenclature': 1, 'nov': 1, 'number': 3, 'numerical': 1, 'nutrient': 1, 'nutrition': 3, 'nutritional': 1, 'obtain': 1, 'october': 1, 'odo': 3, 'of': 27, 'offer': 6, 'often': 2, 'ofthe': 1, 'old': 1, 'oman': 1, 'on': 2, 'one': 10, 'onion': 4, 'only': 1, 'onwards': 1, 'oppose': 1, 'or': 7, 'organism': 3, 'organization': 2, 'originate': 1, 'other': 1, 'our': 1, 'ourselves': 1, 'outer': 1, 'ovary': 1, 'over': 1, 'pack': 5, 'package': 4, 'packaging': 3, 'padua': 1, 'pair': 1, 'palatability': 1, 'pallet': 1, 'participate': 1, 'pea': 1, 'peach': 1, 'pear': 1, 'people': 1, 'pepper': 1, 'per': 7, 'perishable': 1, 'personal': 1, 'philippine': 1, 'physic': 1, 'physiological': 1, 'phytologist': 1, 'phytology': 1, 'pink': 2, 'plant': 3, 'pleasing': 1, 'pleasure': 6, 'plenty': 1, 'pod': 1, 'poisonous': 1, 'pomegranate': 2, 'popular': 1, 'possess': 1, 'practice': 3, 'prehistory': 1, 'premium': 1, 'prepare': 2, 'present': 6, 'preservative': 1, 'primary': 1, 'process': 2, 'produce': 1, 'product': 8, 'professional': 1, 'programming': 1, 'promote': 2, 'protein': 2, 'proteins': 1, 'proud': 6, 'pseudoscientific': 1, 'psychotherapy': 1, 'publication': 1, 'pumpkin': 1, 'punnets10': 2, 'pure': 1, 'purple': 2, 'purplish': 1, 'python': 1, 'qatar': 1, 'quality': 7, 'quantity': 1, 'read': 3, 'recognize': 6, 'recommend': 1, 'red': 7, 'refer': 3, 'relationship': 1, 'relevant': 1, 'remain': 1, 'renowned': 6, 'report': 1, 'reproduce': 1, 'reputed': 1, 'requested': 1, 'requirement': 2, 'richard': 1, 'round': 1, 'rugged': 1, 'russia': 1, 'saudi': 1, 'science': 3, 'scientist': 1, 'seamless': 1, 'second': 1, 'seed': 2, 'seedless': 3, 'sell': 1, 'semantic': 1, 'september': 1, 'serve': 1, 'set': 1, 'seven': 1, 'shade': 1, 'shape': 1, 'sharad': 1, 'shelf': 1, 'similarity': 1, 'singapore': 2, 'single': 1, 'six': 1, 'sixth': 1, 'size': 7, 'skin': 4, 'so': 1, 'soft': 2, 'sometimes': 2, 'specialise': 1, 'specie': 1, 'spherical': 1, 'sport': 1, 'spring': 1, 'squash': 1, 'sri': 1, 'standard': 6, 'state': 2, 'statistic': 1, 'store': 1, 'strength': 1, 'strong': 1, 'study': 2, 'substance': 1, 'suit': 1, 'sum': 1, 'summer': 1, 'supple': 1, 'supplier': 9, 'supply': 1, 'support': 2, 'survive': 1, 'suv': 1, 'sweet': 2, 'system': 1, 'table': 1, 'take': 6, 'taste': 4, 'tax': 1, 'taxonomy': 1, 'term': 1, 'that': 7, 'the': 24, 'thecustomers': 1, 'their': 2, 'these': 4, 'they': 1, 'this': 8, 'thomson': 1, 'thoroughly': 1, 'those': 1, 'through': 1, 'till': 1, 'ting': 2, 'to': 17, 'tomato': 2, 'tool': 2, 'type': 1, 'typically': 1, 'uae': 1, 'uk': 1, 'unit': 1, 'united': 1, 'university': 1, 'unlike': 1, 'unordered': 1, 'use': 8, 'useful': 1, 'usually': 1, 'utility': 1, 'value': 1, 'variety': 3, 'vary': 1, 'vegetable': 1, 'vehicle': 1, 'version': 1, 'vietnam': 1, 'vitamin': 1, 'water': 1, 'we': 11, 'website': 1, 'weight': 2, 'welcome': 1, 'when': 6, 'where': 1, 'which': 10, 'while': 3, 'whilst': 1, 'white': 3, 'who': 1, 'whole': 1, 'wide': 6, 'widely': 2, 'wildtrak': 1, 'will': 1, 'with': 14, 'world': 6, 'wt': 2, 'year': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(word_freq_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGc2FsvrstDw",
        "outputId": "0ba19a9d-7f8e-476f-db60-b4168ceb3a49"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With no preprocessing, size of word_freq_dict is 669. After preprocessing, this dictionary is smaller."
      ],
      "metadata": {
        "id": "rxQ8zvnEvavX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_top_similarity_preprocess(query, top_n=5, stop_words=None):\n",
        "\n",
        "    # preprocessing query\n",
        "    lemmatized_query = lemmatize_document(query)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
        "    vectors = vectorizer.fit_transform([lemmatized_query] + lemmatized_documents)\n",
        "\n",
        "    # Calculate similarity scores\n",
        "    cosine_similarities = linear_kernel(vectors[0:1], vectors).flatten()\n",
        "\n",
        "    # Output the similarity scores for the top N documents\n",
        "    top_indices = cosine_similarities.argsort()[-2:-top_n-2:-1]\n",
        "    print(\"Query:\", query)\n",
        "    print(f\"Top {top_n} similar documents:\")\n",
        "    for idx in top_indices:\n",
        "        print(f\"Document {idx-1}: \\t{documents[idx-1][:80]}, \\tsimilarity score {round(cosine_similarities[idx], 2)}\")\n"
      ],
      "metadata": {
        "id": "tQbGvZdmPucI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for query in queries:\n",
        "  find_top_similarity_preprocess(query)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEhohxUwPuYh",
        "outputId": "85ad5a37-41dd-4380-9081-d59e93f0dacc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: fruits\n",
            "Top 5 similar documents:\n",
            "Document 29: \tFruit dishes are those that use fruit as a primary ingredient. Condiments prepar, \tsimilarity score 0.44\n",
            "Document 6: \tTo a botanist, a fruit is an entity that develops from the fertilized ovary of a, \tsimilarity score 0.23\n",
            "Document 0: \tFresh Pomegranate from Anushka Avni International Bhagwa is a premium Pomegranat, \tsimilarity score 0.15\n",
            "Document 31: \tA fruit serving bowl is a round dish or container typically used to prepare and , \tsimilarity score 0.08\n",
            "Document 1: \tFresh Pomegranate Arakta from Anushka Avni International This Pomegranate are bi, \tsimilarity score 0.05\n",
            "\n",
            "Query: vegetables\n",
            "Top 5 similar documents:\n",
            "Document 10: \tCanada's Food Guide is a nutrition guide produced by Health Canada to promote He, \tsimilarity score 0.07\n",
            "Document 14: \tBerry Size: 18mm and above Packaging Packing Size: 4.5 kg loose in carry bags 8., \tsimilarity score 0.0\n",
            "Document 0: \tFresh Pomegranate from Anushka Avni International Bhagwa is a premium Pomegranat, \tsimilarity score 0.0\n",
            "Document 1: \tFresh Pomegranate Arakta from Anushka Avni International This Pomegranate are bi, \tsimilarity score 0.0\n",
            "Document 2: \tAbout Us Anushka Avni International (AAI) takes pleasure in presenting itself as, \tsimilarity score 0.0\n",
            "\n",
            "Query: healthy foods in Canada\n",
            "Top 5 similar documents:\n",
            "Document 10: \tCanada's Food Guide is a nutrition guide produced by Health Canada to promote He, \tsimilarity score 0.58\n",
            "Document 9: \tIn nutrition, the diet of an organism is the sum of foods it eats, which is larg, \tsimilarity score 0.24\n",
            "Document 12: \tCanadian Industry Statistics (CIS) analyses industry data on many economic indic, \tsimilarity score 0.12\n",
            "Document 31: \tA fruit serving bowl is a round dish or container typically used to prepare and , \tsimilarity score 0.12\n",
            "Document 28: \tThe Ford Bronco is a model line of sport utility vehicles manufactured and marke, \tsimilarity score 0.1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What impact did the text cleaning / preprocessing have on your results?"
      ],
      "metadata": {
        "id": "eg7AQ9wGy1BA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After preprocessing, the scores are a bit different for some queries. For example for query 'fruits', the top documents was Document 6 (score 0.16) without any preprocessing, but after preprocessing, Document 29 is the top document (score, 0.44). For query 'vegetables', the ranking of the documents are not different, neither are the scores. For query 'health foods in Canada', the top 3 documents are the same for both approaches. With preprocessing, it is able to pick up Document 31 and Document 28. Hence, for some queries, some important documents are missed if preprocessing steps did not take place."
      ],
      "metadata": {
        "id": "HnOEiP5bSIOo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wRWtek5oy5tv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8V2kx6Epnxq"
      },
      "source": [
        "# Experiment 2: Semantic matching using GloVe embeddings\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe (Global Vectors) is a model for distributed word representation developed at Stanford and launched in 2014. It is an unsupervised learning algorithm that creates vector representations for words by mapping them into a space where distances reflect semantic similarity. GloVe is trained on global word-word co-occurrence statistics from a corpus, resulting in word vectors that exhibit interesting linear substructures. It combines features of global matrix factorization and local context window methods."
      ],
      "metadata": {
        "id": "AplKLe-615TX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGBg_Roo8FvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ecda11-b495-4ab8-effe-192566234426"
      },
      "source": [
        "# if you decide to use the gensim library and the sample codes below,\n",
        "# you would need gensim version >=4.0.1 to be installed\n",
        "\n",
        "import gensim\n",
        "print(gensim.__version__)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oywLsqBYrayZ"
      },
      "source": [
        "import logging\n",
        "import json\n",
        "import logging\n",
        "from re import sub\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gensim.downloader as api\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
        "from gensim.similarities import SparseTermSimilarityMatrix\n",
        "from gensim.similarities import SoftCosineSimilarity"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa0dlD102lbp"
      },
      "source": [
        "# optional, but it helps\n",
        "import logging\n",
        "\n",
        "# Initialize logging.\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwQBtaxk5rXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56bfd53f-ce81-49ec-b740-a8a844b37fc3"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# Import and download stopwords from NLTK.\n",
        "nltk.download('stopwords')  # Download stopwords list.\n",
        "stopwords = set(nltk.corpus.stopwords.words(\"english\"))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mlGyZeZ2Nle"
      },
      "source": [
        "import re\n",
        "\n",
        "def preprocess(doc):\n",
        "    # Convert to lower case\n",
        "    doc = doc.lower()\n",
        "    # Tokenize, clean up input document string\n",
        "    doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n",
        "    # Remove special characters\n",
        "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc)\n",
        "\n",
        "    # Convert to lower case\n",
        "    doc = doc.lower()\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(\"inf\")) if token not in stopwords]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0eULa5tv3kY3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP1s_4hMpqAn"
      },
      "source": [
        "# Load test data\n",
        "with open(filePath) as in_file:\n",
        "    repo_data = json.load(in_file)\n",
        "\n",
        "titles = [item[0] for item in repo_data['data']]\n",
        "documents = [item[1] for item in repo_data['data']]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries"
      ],
      "metadata": {
        "id": "4fj_43IAanD2",
        "outputId": "b0c8ccb0-5f9f-4012-ed3f-0b5c8bd60655",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fruits', 'vegetables', 'healthy foods in Canada']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaVXOXGDpqD5"
      },
      "source": [
        "# Preprocess the documents, including the query string\n",
        "corpus = [preprocess(document) for document in documents]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR_F5zkipqH7",
        "outputId": "33a1c5e7-c888-491d-e5ff-4a6aac3878d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download and load the GloVe word vector embeddings\n",
        "if 'glove' not in locals():  # only load if not already in memory\n",
        "    glove = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "similarity_index = WordEmbeddingSimilarityIndex(glove)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def glove_find_top_similarity(query, top_n=5):\n",
        "  query = preprocess(query)\n",
        "\n",
        "  # Build the term dictionary, TF-idf model\n",
        "  dictionary = Dictionary(corpus+[query])\n",
        "  tfidf = TfidfModel(dictionary=dictionary)\n",
        "\n",
        "  # Create the term similarity matrix.\n",
        "  similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)\n",
        "\n",
        "  # Compute similarity measure between the query and the documents.\n",
        "  query_tf = tfidf[dictionary.doc2bow(query)]\n",
        "\n",
        "  index = SoftCosineSimilarity(\n",
        "              tfidf[[dictionary.doc2bow(document) for document in corpus]],\n",
        "              similarity_matrix)\n",
        "\n",
        "  doc_similarity_scores = index[query_tf]\n",
        "  print()\n",
        "\n",
        "  # sort documents by similarity scores\n",
        "  sorted_indices = np.argsort(doc_similarity_scores)[::-1]  # Sort indices in descending order\n",
        "  top_n_docs = [(documents[idx], idx, doc_similarity_scores[idx]) for idx in sorted_indices[:5]]\n",
        "\n",
        "  print()\n",
        "  for doc, idx, score in top_n_docs:\n",
        "    print(f\"Document {idx}: \\t{documents[idx][:80]}, \\tsimilarity score {score:.2f}\")"
      ],
      "metadata": {
        "id": "Id829_DYefvZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for query in queries:\n",
        "  print(f\"Query: {query}\")\n",
        "  glove_find_top_similarity(query)\n",
        "  print()"
      ],
      "metadata": {
        "id": "H-bBqMzmefsG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eaa5b9f-b26d-4443-afc7-0d9dda1d55b5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: fruits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 569/569 [00:18<00:00, 31.24it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gensim/similarities/termsim.py:382: RuntimeWarning: divide by zero encountered in divide\n",
            "  normalized_corpus = np.multiply(corpus, 1.0 / corpus_norm)\n",
            "/usr/local/lib/python3.10/dist-packages/gensim/similarities/termsim.py:382: RuntimeWarning: invalid value encountered in multiply\n",
            "  normalized_corpus = np.multiply(corpus, 1.0 / corpus_norm)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Document 6: \tTo a botanist, a fruit is an entity that develops from the fertilized ovary of a, \tsimilarity score 0.88\n",
            "Document 31: \tA fruit serving bowl is a round dish or container typically used to prepare and , \tsimilarity score 0.84\n",
            "Document 29: \tFruit dishes are those that use fruit as a primary ingredient. Condiments prepar, \tsimilarity score 0.84\n",
            "Document 0: \tFresh Pomegranate from Anushka Avni International Bhagwa is a premium Pomegranat, \tsimilarity score 0.79\n",
            "Document 10: \tCanada's Food Guide is a nutrition guide produced by Health Canada to promote He, \tsimilarity score 0.76\n",
            "\n",
            "Query: vegetables\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 569/569 [00:16<00:00, 34.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Document 6: \tTo a botanist, a fruit is an entity that develops from the fertilized ovary of a, \tsimilarity score 0.90\n",
            "Document 10: \tCanada's Food Guide is a nutrition guide produced by Health Canada to promote He, \tsimilarity score 0.81\n",
            "Document 29: \tFruit dishes are those that use fruit as a primary ingredient. Condiments prepar, \tsimilarity score 0.76\n",
            "Document 17: \tWe are one of the leading organizations engaged in delivering our customers with, \tsimilarity score 0.75\n",
            "Document 4: \tWhite Onions from Anushka Avni International Fresh White Onion, which is widely , \tsimilarity score 0.71\n",
            "\n",
            "Query: healthy foods in Canada\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 569/569 [00:14<00:00, 38.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Document 10: \tCanada's Food Guide is a nutrition guide produced by Health Canada to promote He, \tsimilarity score 0.93\n",
            "Document 9: \tIn nutrition, the diet of an organism is the sum of foods it eats, which is larg, \tsimilarity score 0.68\n",
            "Document 31: \tA fruit serving bowl is a round dish or container typically used to prepare and , \tsimilarity score 0.59\n",
            "Document 16: \tAnushka Avni International (AAI) takes pleasure in presenting itself as one of t, \tsimilarity score 0.59\n",
            "Document 0: \tFresh Pomegranate from Anushka Avni International Bhagwa is a premium Pomegranat, \tsimilarity score 0.53\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With GloVe enbeddings, there are consistently higher similarity score for all of the queries, indicating that this approach is able to discern the the graded similarity between documents.\n",
        "\n",
        "For query 'fruits', the top documents are similar to those find at the end of Experiment 2. The order of similarity is a different, all show high degree of similarity (between 0.76 - 0.88). This is different from the result from end of experiment 2. There, the similarity scores ranges from 0.05 - 0.44.\n",
        "\n",
        "This is similar to what is observed for queries 'vegetables' and 'healthy food in Canada'. In conclusion, GloVe embedding may be is better in detecting more subtle difference in the similar of different documents."
      ],
      "metadata": {
        "id": "5enki2hdshDl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8at07gGAI-MH"
      },
      "source": [
        "# Experiment 3: BERT Model\n",
        "***\n",
        "Use a BERT model obtain sentence embeddings and calculate the similarity between queries and documents.\n",
        "\n",
        "> Hint: see the Module 07 jupyter notebook for examples of how to work with BERT."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow"
      ],
      "metadata": {
        "id": "AeCKBJXEb8-0",
        "outputId": "b56ede48-f0ac-49b1-a344-abdce07c17d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.16.2\n",
            "Uninstalling tensorflow-2.16.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow-2.16.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.15.1"
      ],
      "metadata": {
        "id": "uP6hZk65cBRj",
        "outputId": "abf61db6-8d7d-4d48-8abc-192aeb639457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.1\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.3.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.1)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (2.15.0)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.1) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.2.2)\n",
            "Installing collected packages: keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.16.2\n",
            "    Uninstalling tensorboard-2.16.2:\n",
            "      Successfully uninstalled tensorboard-2.16.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.15.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 tensorboard-2.15.2 tensorflow-2.15.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "tensorboard",
                  "tensorflow"
                ]
              },
              "id": "0444ca4256b244d8b392eb03c162a196"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "metadata": {
        "id": "w_NEc_shcC51",
        "outputId": "9027ad3f-31bc-49b5-9098-6570b0d1b29c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub"
      ],
      "metadata": {
        "id": "IsUcUFneYXW0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the  a BERT model to fine-tune\n",
        "# here we use BERT-Base with fewer parameters (Uncased) which was released by the original BERT authors\n",
        "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTMkesVNZQMi",
        "outputId": "345c12cc-8f13-4b69-8525-5db27c0dc72c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9U8JlERYeD_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow_text==2.11.0"
      ],
      "metadata": {
        "id": "IDsZliXDa5Gy",
        "outputId": "89f5f476-ac7b-4930-e139-b91644c12cfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-pubsub 2.21.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "googleapis-common-protos 1.63.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.6 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text"
      ],
      "metadata": {
        "id": "NwqtkbqSa0uF",
        "outputId": "bebc9643-650d-4ee9-eacc-3222463f6468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "/usr/local/lib/python3.10/dist-packages/tensorflow_text/core/pybinds/tflite_registrar.so: undefined symbol: _ZN4absl12lts_2022062320raw_logging_internal21internal_log_functionB5cxx11E",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-4dd33f1b1ed3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_text/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpybinds\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtflite_registrar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.10/dist-packages/tensorflow_text/core/pybinds/tflite_registrar.so: undefined symbol: _ZN4absl12lts_2022062320raw_logging_internal21internal_log_functionB5cxx11E",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the preprocessing model into a hub.KerasLayer to compose the fine-tuned model\n",
        "# with the smallBert The input is truncated to 128 tokens (The number of tokens can be customized)\n",
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ],
      "metadata": {
        "id": "kwjEv3GqZQJU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "4c10ba2e-7e30-4c06-ea1b-ea252487ad11"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Op type not registered 'CaseFoldUTF8' in binary running on ab4d514c0e0a. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib (e.g. `tf.contrib.resampler`), accessing should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mop_def_for_type\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   3016\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3017\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op_def_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3018\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'CaseFoldUTF8'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-cc45ab029350>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the preprocessing model into a hub.KerasLayer to compose the fine-tuned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# with the smallBert The input is truncated to 128 tokens (The number of tokens can be customized)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbert_preprocess_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfhub_handle_preprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Expected before TF2.4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m           \u001b[0mset_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m    124\u001b[0m         module_path, tags=tags, options=options)\n\u001b[1;32m    125\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m   \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0mexport_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         loader = Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0m\u001b[1;32m   1044\u001b[0m                         ckpt_options, options, filters)\n\u001b[1;32m   1045\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_export_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexport_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     self._concrete_functions = (\n\u001b[0;32m--> 161\u001b[0;31m         function_deserialization.load_function_def_library(\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mlibrary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0msaved_object_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/function_deserialization.py\u001b[0m in \u001b[0;36mload_function_def_library\u001b[0;34m(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;31m# import).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       func_graph = function_def_lib.function_def_to_graph(\n\u001b[0m\u001b[1;32m    457\u001b[0m           \u001b[0mfdef\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m           \u001b[0mstructured_input_signature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstructured_input_signature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/function_def_to_graph.py\u001b[0m in \u001b[0;36mfunction_def_to_graph\u001b[0;34m(fdef, structured_input_signature, structured_outputs, input_shapes, propagate_device_spec, include_library_functions)\u001b[0m\n\u001b[1;32m     89\u001b[0m           \u001b[0minput_shapes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m   graph_def, nested_to_flat_tensor_name = function_def_to_graph_def(\n\u001b[0m\u001b[1;32m     92\u001b[0m       \u001b[0mfdef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_library_functions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_library_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/function_def_to_graph.py\u001b[0m in \u001b[0;36mfunction_def_to_graph_def\u001b[0;34m(fdef, input_shapes, include_library_functions)\u001b[0m\n\u001b[1;32m    328\u001b[0m           \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrad_def\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m       \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_def_for_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mop_def_for_type\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3019\u001b[0m       self._op_def_cache[type] = op_def_pb2.OpDef.FromString(\n\u001b[0;32m-> 3020\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op_def_for_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3021\u001b[0m       )\n\u001b[1;32m   3022\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op_def_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Op type not registered 'CaseFoldUTF8' in binary running on ab4d514c0e0a. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib (e.g. `tf.contrib.resampler`), accessing should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute BERT embeddings\n",
        "def compute_bert_embeddings(texts):\n",
        "    preprocessed_text = bert_preprocess_model(texts)\n",
        "    bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
        "    return bert_model(preprocessed_text)[\"pooled_output\"]"
      ],
      "metadata": {
        "id": "4KIPaygoauin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_find_top_similarity(query, top_n=5):\n",
        "  # Compute embeddings for query and documents\n",
        "  query_embedding = compute_bert_embeddings([query])\n",
        "  document_embeddings = [compute_bert_embeddings([doc]) for doc in documents]\n",
        "\n",
        "  # Calculate cosine similarity between query and documents\n",
        "  similarity_scores = []\n",
        "  for doc_emb in document_embeddings:\n",
        "      dot_product = tf.reduce_sum(query_embedding * doc_emb)\n",
        "      query_norm = tf.linalg.norm(query_embedding)\n",
        "      doc_norm = tf.linalg.norm(doc_emb)\n",
        "      cosine_sim = dot_product / (query_norm * doc_norm)\n",
        "      similarity_scores.append(cosine_sim.numpy())\n",
        "\n",
        "  # Create a list of tuples (index, score) for sorting\n",
        "  scored_documents = [(idx, score) for idx, score in enumerate(similarity_scores)]\n",
        "\n",
        "  # Sort documents by similarity score in descending order\n",
        "  scored_documents.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  # Print top_n documents with indices and similarity scores\n",
        "  print(f\"Query: {query}\\n\")\n",
        "  for rank, (idx, score) in enumerate(scored_documents[:top_n], 1):\n",
        "    print(f\"Document {idx}: \\t{documents[idx][:80]}, \\tsimilarity score {score:.2f}\")\n"
      ],
      "metadata": {
        "id": "k4oftll4p5Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for query in queries:\n",
        "  bert_find_top_similarity(query)\n",
        "  print()"
      ],
      "metadata": {
        "id": "H3km-xfmquOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Technique Comparison\n",
        " ***\n",
        "\n",
        "Compare all three techniques and interpret your findings. Do your best to explain the differences you observe in terms of concepts learned in class (not just the what, but also the how and why one technique produces different results from another).\n"
      ],
      "metadata": {
        "id": "XpX8H6_TEPOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach 1: TF-IDF\n",
        "\n",
        "In this approach, I used TF-IDF, which converts each document into a TF-IDF vector. It takes into account the term frequency (TF) within the document and the inverse document frequency (IDF), which measures how common or rare a word is across all documents. The TF-IDF vectors for each document are used to assess the similarity of the documents based on the cosine similarity of these vectors. This approach does not consider syntax or semantic similarity.\n",
        "\n",
        "Approach 2: GloVe Embedding\n",
        "\n",
        "In this approach, I used GloVe embeddings, which are based on co-occurrence statistics in a large corpus. This method captures semantic relationships between words, but it does not consider contextual information. This approach was able to uncover some documents that were not detected using approach 1.\n",
        "\n",
        "Approach 3: BERT Embedding\n",
        "\n",
        "In this approach, I used BERT embeddings. BERT takes into account the meaning of words based on their context in a sentence or document. This is a better approach for capturing nuanced meanings and context-specific semantics.\n",
        "\n",
        "Using BERT embeddings, I was able to uncover documents not identified in approaches 1 and 2. This approach seems to be better for document classification. I noticed that computation was fastest for approach 1, followed by approach 2, with BERT embeddings being the slowest, which is expected due to its deep architecture.\n"
      ],
      "metadata": {
        "id": "XRPjUZUOLXQH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LwKekAZX1kDj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}